{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "import joblib\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plot\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"database_intents.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar get_dummies en vez de las 3 funciones\n",
    "df_oh = pd.concat([df, pd.get_dummies(df['Intent type'])], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_oh, train_size = 0.7, test_size = 0.3, random_state = 42,\n",
    "                                    shuffle = True, stratify = df_oh['Intent type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(df, pretreatment = False, Tfidf = True, cv = None, stopwords = []):\n",
    "  # Normalizamos y limpiamos el corpus \n",
    "  if pretreatment == True:\n",
    "    df[\"Sentence\"] = df['Sentence'].apply(lambda x: word_treatment(x))\n",
    "    print(\"El corpus ha sido pretratado\")\n",
    "    \n",
    "     # Transformamos nuestro corpus a un vector Tfidf\n",
    "  if Tfidf == True:\n",
    "\n",
    "    if cv == None:\n",
    "      cv = TfidfVectorizer(\n",
    "        stop_words= stopwords,\n",
    "        ngram_range=(1, 4),\n",
    "        strip_accents='ascii',\n",
    "        max_df=0.99,\n",
    "        min_df=0,\n",
    "        max_features=100\n",
    "      )\n",
    "      cv.fit(df[\"Sentence\"])\n",
    "      X = cv.transform(df[\"Sentence\"])\n",
    "      print(\"Se ha realizado una vectorización Tfidf\")\n",
    "      return df, X, cv\n",
    "\n",
    "    else:\n",
    "      X = cv.transform(df[\"Sentence\"])\n",
    "      #print(\"Se ha realizado una vectorización Tfidf basado en el corpus suministrado por cv\")\n",
    "    return df, X\n",
    "  else:\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha realizado una vectorización Tfidf\n"
     ]
    }
   ],
   "source": [
    "df_train, X_train, cv = processing(df_train)\n",
    "df_test, X_test = processing(df_test, cv = cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train[[\"Greeting\",\"Search\",\"Suggestions\"]]\n",
    "y_test = df_test[[\"Greeting\",\"Search\",\"Suggestions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, train_size=0.7, \n",
    "test_size=0.3, random_state=42, shuffle=True, stratify=df_train[\"Intent type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape= X_train.shape[1]\n",
    "\n",
    "X_train.sort_indices()\n",
    "X_validation.sort_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_greeting(shape):\n",
    "# define our MLP network\n",
    "    initializer = tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5, seed=42)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=shape, kernel_initializer = initializer, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(2, activation=\"relu\"))\n",
    "# check to see if the regression node should be added\n",
    "    #if regress:\n",
    "    model.add(Dense(1, activation = \"sigmoid\"))\n",
    "    #Compile model\n",
    "    opt = tf.keras.optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss='binary_crossentropy', metrics = [\"accuracy\"], optimizer=opt)\n",
    "# return our model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58 samples, validate on 26 samples\n",
      "Epoch 1/200\n",
      " - 4s - loss: 0.6913 - acc: 0.5517 - val_loss: 0.6899 - val_acc: 0.5769\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.6893 - acc: 0.4655 - val_loss: 0.6864 - val_acc: 0.6154\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.6851 - acc: 0.6207 - val_loss: 0.6830 - val_acc: 0.6923\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.6771 - acc: 0.7241 - val_loss: 0.6794 - val_acc: 0.7308\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.6778 - acc: 0.5690 - val_loss: 0.6759 - val_acc: 0.7308\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.6793 - acc: 0.6552 - val_loss: 0.6723 - val_acc: 0.7308\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.6733 - acc: 0.7414 - val_loss: 0.6688 - val_acc: 0.7308\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.6731 - acc: 0.7069 - val_loss: 0.6653 - val_acc: 0.7308\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.6769 - acc: 0.7241 - val_loss: 0.6618 - val_acc: 0.7308\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.6859 - acc: 0.7241 - val_loss: 0.6583 - val_acc: 0.7308\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.6600 - acc: 0.7414 - val_loss: 0.6549 - val_acc: 0.7308\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.6452 - acc: 0.7586 - val_loss: 0.6512 - val_acc: 0.7308\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.6493 - acc: 0.7414 - val_loss: 0.6476 - val_acc: 0.7308\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.6443 - acc: 0.7414 - val_loss: 0.6438 - val_acc: 0.7308\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.6480 - acc: 0.7241 - val_loss: 0.6402 - val_acc: 0.7308\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.6306 - acc: 0.7759 - val_loss: 0.6365 - val_acc: 0.7308\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.6403 - acc: 0.7586 - val_loss: 0.6328 - val_acc: 0.7308\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.6250 - acc: 0.7586 - val_loss: 0.6290 - val_acc: 0.7308\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.6277 - acc: 0.7414 - val_loss: 0.6251 - val_acc: 0.7308\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.6113 - acc: 0.7759 - val_loss: 0.6211 - val_acc: 0.7308\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.6266 - acc: 0.7759 - val_loss: 0.6170 - val_acc: 0.7308\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.6174 - acc: 0.7586 - val_loss: 0.6130 - val_acc: 0.7308\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.6081 - acc: 0.7586 - val_loss: 0.6089 - val_acc: 0.7308\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.6152 - acc: 0.7586 - val_loss: 0.6048 - val_acc: 0.7308\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.6086 - acc: 0.7586 - val_loss: 0.6007 - val_acc: 0.7308\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.5854 - acc: 0.7586 - val_loss: 0.5963 - val_acc: 0.7308\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.5904 - acc: 0.7241 - val_loss: 0.5919 - val_acc: 0.7308\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.5500 - acc: 0.7586 - val_loss: 0.5874 - val_acc: 0.7308\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.5749 - acc: 0.7586 - val_loss: 0.5828 - val_acc: 0.7308\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.5819 - acc: 0.7586 - val_loss: 0.5784 - val_acc: 0.7308\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.5816 - acc: 0.7586 - val_loss: 0.5741 - val_acc: 0.7308\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.5574 - acc: 0.7586 - val_loss: 0.5700 - val_acc: 0.7308\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.5535 - acc: 0.7586 - val_loss: 0.5658 - val_acc: 0.7308\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.5650 - acc: 0.7586 - val_loss: 0.5616 - val_acc: 0.7308\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.5678 - acc: 0.7586 - val_loss: 0.5576 - val_acc: 0.7308\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.5646 - acc: 0.7586 - val_loss: 0.5537 - val_acc: 0.7308\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.5556 - acc: 0.7586 - val_loss: 0.5499 - val_acc: 0.7308\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.5792 - acc: 0.7586 - val_loss: 0.5463 - val_acc: 0.7308\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.4889 - acc: 0.7586 - val_loss: 0.5426 - val_acc: 0.7308\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.5403 - acc: 0.7586 - val_loss: 0.5387 - val_acc: 0.7308\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.5599 - acc: 0.7586 - val_loss: 0.5349 - val_acc: 0.7308\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.5452 - acc: 0.7586 - val_loss: 0.5313 - val_acc: 0.7308\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.5122 - acc: 0.7586 - val_loss: 0.5278 - val_acc: 0.7308\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.5563 - acc: 0.7586 - val_loss: 0.5243 - val_acc: 0.7308\n",
      "Epoch 45/200\n",
      " - 0s - loss: 0.4863 - acc: 0.7586 - val_loss: 0.5209 - val_acc: 0.7308\n",
      "Epoch 46/200\n",
      " - 0s - loss: 0.5243 - acc: 0.7586 - val_loss: 0.5175 - val_acc: 0.7308\n",
      "Epoch 47/200\n",
      " - 0s - loss: 0.5401 - acc: 0.7586 - val_loss: 0.5141 - val_acc: 0.7308\n",
      "Epoch 48/200\n",
      " - 0s - loss: 0.4997 - acc: 0.7586 - val_loss: 0.5108 - val_acc: 0.7308\n",
      "Epoch 49/200\n",
      " - 0s - loss: 0.5221 - acc: 0.7586 - val_loss: 0.5074 - val_acc: 0.7308\n",
      "Epoch 50/200\n",
      " - 0s - loss: 0.4809 - acc: 0.7586 - val_loss: 0.5042 - val_acc: 0.7308\n",
      "Epoch 51/200\n",
      " - 0s - loss: 0.5218 - acc: 0.7586 - val_loss: 0.5011 - val_acc: 0.7308\n",
      "Epoch 52/200\n",
      " - 0s - loss: 0.5190 - acc: 0.7586 - val_loss: 0.4979 - val_acc: 0.7308\n",
      "Epoch 53/200\n",
      " - 0s - loss: 0.4728 - acc: 0.7586 - val_loss: 0.4949 - val_acc: 0.7308\n",
      "Epoch 54/200\n",
      " - 0s - loss: 0.4767 - acc: 0.7586 - val_loss: 0.4920 - val_acc: 0.7308\n",
      "Epoch 55/200\n",
      " - 0s - loss: 0.5080 - acc: 0.7586 - val_loss: 0.4890 - val_acc: 0.7308\n",
      "Epoch 56/200\n",
      " - 0s - loss: 0.4806 - acc: 0.7586 - val_loss: 0.4862 - val_acc: 0.7308\n",
      "Epoch 57/200\n",
      " - 0s - loss: 0.4846 - acc: 0.7586 - val_loss: 0.4835 - val_acc: 0.7308\n",
      "Epoch 58/200\n",
      " - 0s - loss: 0.4627 - acc: 0.7586 - val_loss: 0.4807 - val_acc: 0.7308\n",
      "Epoch 59/200\n",
      " - 0s - loss: 0.4681 - acc: 0.7586 - val_loss: 0.4780 - val_acc: 0.7308\n",
      "Epoch 60/200\n",
      " - 0s - loss: 0.4769 - acc: 0.7586 - val_loss: 0.4754 - val_acc: 0.7308\n",
      "Epoch 61/200\n",
      " - 0s - loss: 0.5114 - acc: 0.7586 - val_loss: 0.4729 - val_acc: 0.7308\n",
      "Epoch 62/200\n",
      " - 0s - loss: 0.4829 - acc: 0.7586 - val_loss: 0.4704 - val_acc: 0.7308\n",
      "Epoch 63/200\n",
      " - 0s - loss: 0.4591 - acc: 0.7586 - val_loss: 0.4680 - val_acc: 0.7308\n",
      "Epoch 64/200\n",
      " - 0s - loss: 0.4660 - acc: 0.7586 - val_loss: 0.4655 - val_acc: 0.7308\n",
      "Epoch 65/200\n",
      " - 0s - loss: 0.4578 - acc: 0.7586 - val_loss: 0.4631 - val_acc: 0.7308\n",
      "Epoch 66/200\n",
      " - 0s - loss: 0.4622 - acc: 0.7586 - val_loss: 0.4607 - val_acc: 0.7308\n",
      "Epoch 67/200\n",
      " - 0s - loss: 0.4936 - acc: 0.7586 - val_loss: 0.4584 - val_acc: 0.7308\n",
      "Epoch 68/200\n",
      " - 0s - loss: 0.4430 - acc: 0.7586 - val_loss: 0.4561 - val_acc: 0.7308\n",
      "Epoch 69/200\n",
      " - 0s - loss: 0.4565 - acc: 0.7586 - val_loss: 0.4536 - val_acc: 0.7308\n",
      "Epoch 70/200\n",
      " - 0s - loss: 0.4309 - acc: 0.7586 - val_loss: 0.4511 - val_acc: 0.7308\n",
      "Epoch 71/200\n",
      " - 0s - loss: 0.3740 - acc: 0.7586 - val_loss: 0.4485 - val_acc: 0.7308\n",
      "Epoch 72/200\n",
      " - 0s - loss: 0.4611 - acc: 0.7586 - val_loss: 0.4459 - val_acc: 0.7308\n",
      "Epoch 73/200\n",
      " - 0s - loss: 0.3995 - acc: 0.7586 - val_loss: 0.4433 - val_acc: 0.7308\n",
      "Epoch 74/200\n",
      " - 0s - loss: 0.4755 - acc: 0.7586 - val_loss: 0.4408 - val_acc: 0.7308\n",
      "Epoch 75/200\n",
      " - 0s - loss: 0.4530 - acc: 0.7586 - val_loss: 0.4383 - val_acc: 0.7308\n",
      "Epoch 76/200\n",
      " - 0s - loss: 0.4915 - acc: 0.7586 - val_loss: 0.4359 - val_acc: 0.7308\n",
      "Epoch 77/200\n",
      " - 0s - loss: 0.4250 - acc: 0.7586 - val_loss: 0.4335 - val_acc: 0.7308\n",
      "Epoch 78/200\n",
      " - 0s - loss: 0.4359 - acc: 0.7586 - val_loss: 0.4312 - val_acc: 0.7308\n",
      "Epoch 79/200\n",
      " - 0s - loss: 0.4820 - acc: 0.7586 - val_loss: 0.4290 - val_acc: 0.7308\n",
      "Epoch 80/200\n",
      " - 0s - loss: 0.4336 - acc: 0.7586 - val_loss: 0.4269 - val_acc: 0.7308\n",
      "Epoch 81/200\n",
      " - 0s - loss: 0.4145 - acc: 0.7586 - val_loss: 0.4247 - val_acc: 0.7308\n",
      "Epoch 82/200\n",
      " - 0s - loss: 0.4379 - acc: 0.7586 - val_loss: 0.4225 - val_acc: 0.7308\n",
      "Epoch 83/200\n",
      " - 0s - loss: 0.4147 - acc: 0.7586 - val_loss: 0.4203 - val_acc: 0.7308\n",
      "Epoch 84/200\n",
      " - 0s - loss: 0.4157 - acc: 0.7586 - val_loss: 0.4183 - val_acc: 0.7308\n",
      "Epoch 85/200\n",
      " - 0s - loss: 0.4466 - acc: 0.7586 - val_loss: 0.4163 - val_acc: 0.7308\n",
      "Epoch 86/200\n",
      " - 0s - loss: 0.4507 - acc: 0.7586 - val_loss: 0.4144 - val_acc: 0.7308\n",
      "Epoch 87/200\n",
      " - 0s - loss: 0.4045 - acc: 0.7586 - val_loss: 0.4127 - val_acc: 0.7308\n",
      "Epoch 88/200\n",
      " - 0s - loss: 0.4173 - acc: 0.7586 - val_loss: 0.4110 - val_acc: 0.7308\n",
      "Epoch 89/200\n",
      " - 0s - loss: 0.4814 - acc: 0.7586 - val_loss: 0.4092 - val_acc: 0.7308\n",
      "Epoch 90/200\n",
      " - 0s - loss: 0.4174 - acc: 0.7586 - val_loss: 0.4072 - val_acc: 0.7308\n",
      "Epoch 91/200\n",
      " - 0s - loss: 0.3842 - acc: 0.7586 - val_loss: 0.4054 - val_acc: 0.7308\n",
      "Epoch 92/200\n",
      " - 0s - loss: 0.4407 - acc: 0.7586 - val_loss: 0.4036 - val_acc: 0.7308\n",
      "Epoch 93/200\n",
      " - 0s - loss: 0.3475 - acc: 0.7586 - val_loss: 0.4018 - val_acc: 0.7308\n",
      "Epoch 94/200\n",
      " - 0s - loss: 0.3981 - acc: 0.7586 - val_loss: 0.4001 - val_acc: 0.7308\n",
      "Epoch 95/200\n",
      " - 0s - loss: 0.3804 - acc: 0.7586 - val_loss: 0.3984 - val_acc: 0.7308\n",
      "Epoch 96/200\n",
      " - 0s - loss: 0.3532 - acc: 0.7586 - val_loss: 0.3968 - val_acc: 0.7308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/200\n",
      " - 0s - loss: 0.4279 - acc: 0.7586 - val_loss: 0.3951 - val_acc: 0.7308\n",
      "Epoch 98/200\n",
      " - 0s - loss: 0.4278 - acc: 0.7586 - val_loss: 0.3935 - val_acc: 0.7308\n",
      "Epoch 99/200\n",
      " - 0s - loss: 0.4057 - acc: 0.7586 - val_loss: 0.3919 - val_acc: 0.7308\n",
      "Epoch 100/200\n",
      " - 0s - loss: 0.4097 - acc: 0.7586 - val_loss: 0.3904 - val_acc: 0.7308\n",
      "Epoch 101/200\n",
      " - 0s - loss: 0.4071 - acc: 0.7586 - val_loss: 0.3889 - val_acc: 0.7308\n",
      "Epoch 102/200\n",
      " - 0s - loss: 0.3503 - acc: 0.7586 - val_loss: 0.3873 - val_acc: 0.7308\n",
      "Epoch 103/200\n",
      " - 0s - loss: 0.3668 - acc: 0.7586 - val_loss: 0.3858 - val_acc: 0.7308\n",
      "Epoch 104/200\n",
      " - 0s - loss: 0.3897 - acc: 0.7586 - val_loss: 0.3841 - val_acc: 0.7308\n",
      "Epoch 105/200\n",
      " - 0s - loss: 0.3703 - acc: 0.7586 - val_loss: 0.3825 - val_acc: 0.7308\n",
      "Epoch 106/200\n",
      " - 0s - loss: 0.3432 - acc: 0.7586 - val_loss: 0.3810 - val_acc: 0.7308\n",
      "Epoch 107/200\n",
      " - 0s - loss: 0.3487 - acc: 0.7586 - val_loss: 0.3795 - val_acc: 0.7308\n",
      "Epoch 108/200\n",
      " - 0s - loss: 0.4168 - acc: 0.7586 - val_loss: 0.3781 - val_acc: 0.7308\n",
      "Epoch 109/200\n",
      " - 0s - loss: 0.3558 - acc: 0.7586 - val_loss: 0.3768 - val_acc: 0.7308\n",
      "Epoch 110/200\n",
      " - 0s - loss: 0.4285 - acc: 0.7586 - val_loss: 0.3753 - val_acc: 0.7308\n",
      "Epoch 111/200\n",
      " - 0s - loss: 0.3254 - acc: 0.7586 - val_loss: 0.3739 - val_acc: 0.7308\n",
      "Epoch 112/200\n",
      " - 0s - loss: 0.3649 - acc: 0.7586 - val_loss: 0.3726 - val_acc: 0.7308\n",
      "Epoch 113/200\n",
      " - 0s - loss: 0.4093 - acc: 0.7586 - val_loss: 0.3712 - val_acc: 0.7308\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.4134 - acc: 0.7586 - val_loss: 0.3702 - val_acc: 0.7308\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.3810 - acc: 0.7586 - val_loss: 0.3693 - val_acc: 0.7308\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.3830 - acc: 0.7586 - val_loss: 0.3684 - val_acc: 0.7308\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.4259 - acc: 0.7586 - val_loss: 0.3676 - val_acc: 0.7308\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.3828 - acc: 0.7586 - val_loss: 0.3670 - val_acc: 0.7308\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.3452 - acc: 0.7586 - val_loss: 0.3664 - val_acc: 0.7308\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.3171 - acc: 0.7586 - val_loss: 0.3656 - val_acc: 0.7308\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.3376 - acc: 0.7586 - val_loss: 0.3648 - val_acc: 0.7308\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.3396 - acc: 0.7586 - val_loss: 0.3641 - val_acc: 0.7308\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.3638 - acc: 0.7586 - val_loss: 0.3634 - val_acc: 0.7308\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.3705 - acc: 0.7586 - val_loss: 0.3627 - val_acc: 0.7308\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.3552 - acc: 0.7586 - val_loss: 0.3621 - val_acc: 0.7308\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.3480 - acc: 0.7586 - val_loss: 0.3617 - val_acc: 0.7308\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.3601 - acc: 0.7586 - val_loss: 0.3613 - val_acc: 0.7308\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.3657 - acc: 0.7586 - val_loss: 0.3611 - val_acc: 0.7308\n",
      "Epoch 129/200\n",
      " - 0s - loss: 0.3777 - acc: 0.7586 - val_loss: 0.3610 - val_acc: 0.7308\n",
      "Epoch 130/200\n",
      " - 0s - loss: 0.3737 - acc: 0.7586 - val_loss: 0.3610 - val_acc: 0.7308\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.3760 - acc: 0.7586 - val_loss: 0.3613 - val_acc: 0.7308\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.3475 - acc: 0.7586 - val_loss: 0.3615 - val_acc: 0.7308\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.3425 - acc: 0.7586 - val_loss: 0.3619 - val_acc: 0.7308\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.2993 - acc: 0.7586 - val_loss: 0.3623 - val_acc: 0.7308\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.3356 - acc: 0.7586 - val_loss: 0.3627 - val_acc: 0.7308\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.3237 - acc: 0.7586 - val_loss: 0.3627 - val_acc: 0.7308\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.3739 - acc: 0.7586 - val_loss: 0.3627 - val_acc: 0.7308\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.3412 - acc: 0.7586 - val_loss: 0.3624 - val_acc: 0.7308\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.2838 - acc: 0.7586 - val_loss: 0.3624 - val_acc: 0.7308\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.3584 - acc: 0.7586 - val_loss: 0.3625 - val_acc: 0.7308\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.3090 - acc: 0.7586 - val_loss: 0.3629 - val_acc: 0.7308\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.3034 - acc: 0.7586 - val_loss: 0.3633 - val_acc: 0.7308\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.2686 - acc: 0.7586 - val_loss: 0.3638 - val_acc: 0.7308\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.3042 - acc: 0.7586 - val_loss: 0.3642 - val_acc: 0.7308\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.3047 - acc: 0.7586 - val_loss: 0.3644 - val_acc: 0.7308\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.3322 - acc: 0.7586 - val_loss: 0.3647 - val_acc: 0.7308\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.2936 - acc: 0.7586 - val_loss: 0.3649 - val_acc: 0.7308\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.2930 - acc: 0.7586 - val_loss: 0.3650 - val_acc: 0.7308\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.2944 - acc: 0.7586 - val_loss: 0.3652 - val_acc: 0.7308\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.2764 - acc: 0.7586 - val_loss: 0.3654 - val_acc: 0.7308\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.2996 - acc: 0.7586 - val_loss: 0.3652 - val_acc: 0.7308\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.3028 - acc: 0.7586 - val_loss: 0.3650 - val_acc: 0.7308\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.2756 - acc: 0.7586 - val_loss: 0.3646 - val_acc: 0.7308\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.2757 - acc: 0.7586 - val_loss: 0.3641 - val_acc: 0.7308\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.2563 - acc: 0.7586 - val_loss: 0.3637 - val_acc: 0.7308\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.2598 - acc: 0.7586 - val_loss: 0.3636 - val_acc: 0.7308\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.2647 - acc: 0.7586 - val_loss: 0.3636 - val_acc: 0.7308\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.3243 - acc: 0.7586 - val_loss: 0.3638 - val_acc: 0.7308\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.2689 - acc: 0.7586 - val_loss: 0.3639 - val_acc: 0.7308\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.2781 - acc: 0.7586 - val_loss: 0.3639 - val_acc: 0.7308\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.2673 - acc: 0.7586 - val_loss: 0.3646 - val_acc: 0.7308\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.2691 - acc: 0.7586 - val_loss: 0.3653 - val_acc: 0.7308\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.2675 - acc: 0.7586 - val_loss: 0.3661 - val_acc: 0.7308\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.2591 - acc: 0.7586 - val_loss: 0.3674 - val_acc: 0.7308\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.2643 - acc: 0.7586 - val_loss: 0.3686 - val_acc: 0.7308\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.2614 - acc: 0.7586 - val_loss: 0.3699 - val_acc: 0.7308\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.2574 - acc: 0.7586 - val_loss: 0.3713 - val_acc: 0.7308\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.2983 - acc: 0.7586 - val_loss: 0.3723 - val_acc: 0.7308\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.2243 - acc: 0.7586 - val_loss: 0.3730 - val_acc: 0.7308\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.2588 - acc: 0.7586 - val_loss: 0.3738 - val_acc: 0.7308\n",
      "Epoch 171/200\n",
      " - 0s - loss: 0.2524 - acc: 0.7586 - val_loss: 0.3743 - val_acc: 0.7308\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.2897 - acc: 0.7586 - val_loss: 0.3745 - val_acc: 0.7308\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.2275 - acc: 0.7586 - val_loss: 0.3748 - val_acc: 0.7308\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.2830 - acc: 0.7586 - val_loss: 0.3757 - val_acc: 0.7308\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.2356 - acc: 0.7586 - val_loss: 0.3766 - val_acc: 0.7308\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.2788 - acc: 0.7586 - val_loss: 0.3774 - val_acc: 0.7308\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.2430 - acc: 0.7586 - val_loss: 0.3781 - val_acc: 0.7308\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.2591 - acc: 0.7586 - val_loss: 0.3791 - val_acc: 0.7308\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.2584 - acc: 0.7586 - val_loss: 0.3806 - val_acc: 0.7308\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.2406 - acc: 0.7586 - val_loss: 0.3827 - val_acc: 0.7308\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.2517 - acc: 0.7586 - val_loss: 0.3850 - val_acc: 0.7308\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.2351 - acc: 0.7586 - val_loss: 0.3874 - val_acc: 0.7308\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.2635 - acc: 0.7586 - val_loss: 0.3893 - val_acc: 0.7308\n",
      "Epoch 184/200\n",
      " - 0s - loss: 0.2173 - acc: 0.7586 - val_loss: 0.3911 - val_acc: 0.7308\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.2529 - acc: 0.7586 - val_loss: 0.3918 - val_acc: 0.7308\n",
      "Epoch 186/200\n",
      " - 0s - loss: 0.2400 - acc: 0.7586 - val_loss: 0.3922 - val_acc: 0.7308\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.2106 - acc: 0.7586 - val_loss: 0.3931 - val_acc: 0.7308\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.2352 - acc: 0.7586 - val_loss: 0.3938 - val_acc: 0.7308\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.2208 - acc: 0.7586 - val_loss: 0.3949 - val_acc: 0.7308\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.2660 - acc: 0.7586 - val_loss: 0.3960 - val_acc: 0.7308\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.2477 - acc: 0.7586 - val_loss: 0.3971 - val_acc: 0.7308\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.2256 - acc: 0.7931 - val_loss: 0.3983 - val_acc: 0.8846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/200\n",
      " - 0s - loss: 0.2154 - acc: 0.8966 - val_loss: 0.3993 - val_acc: 0.8846\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.2193 - acc: 0.8966 - val_loss: 0.4000 - val_acc: 0.8846\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.2245 - acc: 0.9138 - val_loss: 0.4007 - val_acc: 0.8846\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.2346 - acc: 0.9138 - val_loss: 0.4016 - val_acc: 0.8846\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.2421 - acc: 0.9138 - val_loss: 0.4028 - val_acc: 0.8846\n",
      "Epoch 198/200\n",
      " - 0s - loss: 0.2234 - acc: 0.9483 - val_loss: 0.4049 - val_acc: 0.8846\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.2032 - acc: 0.9310 - val_loss: 0.4067 - val_acc: 0.8846\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.2229 - acc: 0.9483 - val_loss: 0.4075 - val_acc: 0.8846\n"
     ]
    }
   ],
   "source": [
    "mlp_greeting = mlp_greeting(shape)\n",
    "history = mlp_greeting.fit(X_train, np.asarray(y_train[\"Greeting\"]).reshape(-1,1),\n",
    "                  validation_data=(X_validation, np.asarray(y_validation[\"Greeting\"]).reshape(-1,1)),\n",
    "    epochs=200,\n",
    "    workers = 2, use_multiprocessing= True, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_search(shape):\n",
    "# define our MLP network\n",
    "    initializer = tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5, seed=42)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=shape, kernel_initializer = initializer, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(2, activation=\"relu\"))\n",
    "# check to see if the regression node should be added\n",
    "    #if regress:\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    #Compile model\n",
    "    opt = tf.keras.optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss='binary_crossentropy', metrics = [\"accuracy\"], optimizer=opt)\n",
    "# return our model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58 samples, validate on 26 samples\n",
      "Epoch 1/250\n",
      " - 1s - loss: 0.6952 - acc: 0.6207 - val_loss: 0.6927 - val_acc: 0.7692\n",
      "Epoch 2/250\n",
      " - 0s - loss: 0.6958 - acc: 0.6897 - val_loss: 0.6923 - val_acc: 0.7692\n",
      "Epoch 3/250\n",
      " - 0s - loss: 0.6937 - acc: 0.6897 - val_loss: 0.6917 - val_acc: 0.7692\n",
      "Epoch 4/250\n",
      " - 0s - loss: 0.6916 - acc: 0.7586 - val_loss: 0.6911 - val_acc: 0.7692\n",
      "Epoch 5/250\n",
      " - 0s - loss: 0.6910 - acc: 0.7586 - val_loss: 0.6905 - val_acc: 0.7692\n",
      "Epoch 6/250\n",
      " - 0s - loss: 0.6913 - acc: 0.7241 - val_loss: 0.6899 - val_acc: 0.7692\n",
      "Epoch 7/250\n",
      " - 0s - loss: 0.6912 - acc: 0.7069 - val_loss: 0.6893 - val_acc: 0.7692\n",
      "Epoch 8/250\n",
      " - 0s - loss: 0.6898 - acc: 0.7414 - val_loss: 0.6886 - val_acc: 0.7692\n",
      "Epoch 9/250\n",
      " - 0s - loss: 0.6902 - acc: 0.7241 - val_loss: 0.6880 - val_acc: 0.7692\n",
      "Epoch 10/250\n",
      " - 0s - loss: 0.6881 - acc: 0.7586 - val_loss: 0.6873 - val_acc: 0.7692\n",
      "Epoch 11/250\n",
      " - 0s - loss: 0.6872 - acc: 0.7586 - val_loss: 0.6866 - val_acc: 0.7692\n",
      "Epoch 12/250\n",
      " - 0s - loss: 0.6862 - acc: 0.7414 - val_loss: 0.6859 - val_acc: 0.7692\n",
      "Epoch 13/250\n",
      " - 0s - loss: 0.6848 - acc: 0.7586 - val_loss: 0.6850 - val_acc: 0.7692\n",
      "Epoch 14/250\n",
      " - 0s - loss: 0.6831 - acc: 0.7586 - val_loss: 0.6839 - val_acc: 0.7692\n",
      "Epoch 15/250\n",
      " - 0s - loss: 0.6847 - acc: 0.7586 - val_loss: 0.6827 - val_acc: 0.7692\n",
      "Epoch 16/250\n",
      " - 0s - loss: 0.6811 - acc: 0.7586 - val_loss: 0.6814 - val_acc: 0.7692\n",
      "Epoch 17/250\n",
      " - 0s - loss: 0.6824 - acc: 0.7586 - val_loss: 0.6801 - val_acc: 0.7692\n",
      "Epoch 18/250\n",
      " - 0s - loss: 0.6812 - acc: 0.7586 - val_loss: 0.6788 - val_acc: 0.7692\n",
      "Epoch 19/250\n",
      " - 0s - loss: 0.6786 - acc: 0.7586 - val_loss: 0.6776 - val_acc: 0.7692\n",
      "Epoch 20/250\n",
      " - 0s - loss: 0.6806 - acc: 0.7414 - val_loss: 0.6765 - val_acc: 0.7692\n",
      "Epoch 21/250\n",
      " - 0s - loss: 0.6773 - acc: 0.7586 - val_loss: 0.6752 - val_acc: 0.7692\n",
      "Epoch 22/250\n",
      " - 0s - loss: 0.6767 - acc: 0.7586 - val_loss: 0.6737 - val_acc: 0.7692\n",
      "Epoch 23/250\n",
      " - 0s - loss: 0.6766 - acc: 0.7586 - val_loss: 0.6723 - val_acc: 0.7692\n",
      "Epoch 24/250\n",
      " - 0s - loss: 0.6715 - acc: 0.7586 - val_loss: 0.6709 - val_acc: 0.7692\n",
      "Epoch 25/250\n",
      " - 0s - loss: 0.6700 - acc: 0.7586 - val_loss: 0.6694 - val_acc: 0.7692\n",
      "Epoch 26/250\n",
      " - 0s - loss: 0.6739 - acc: 0.7586 - val_loss: 0.6681 - val_acc: 0.7692\n",
      "Epoch 27/250\n",
      " - 0s - loss: 0.6700 - acc: 0.7586 - val_loss: 0.6668 - val_acc: 0.7692\n",
      "Epoch 28/250\n",
      " - 0s - loss: 0.6712 - acc: 0.7586 - val_loss: 0.6656 - val_acc: 0.7692\n",
      "Epoch 29/250\n",
      " - 0s - loss: 0.6679 - acc: 0.7586 - val_loss: 0.6643 - val_acc: 0.7692\n",
      "Epoch 30/250\n",
      " - 0s - loss: 0.6675 - acc: 0.7586 - val_loss: 0.6629 - val_acc: 0.7692\n",
      "Epoch 31/250\n",
      " - 0s - loss: 0.6654 - acc: 0.7586 - val_loss: 0.6613 - val_acc: 0.7692\n",
      "Epoch 32/250\n",
      " - 0s - loss: 0.6641 - acc: 0.7586 - val_loss: 0.6599 - val_acc: 0.7692\n",
      "Epoch 33/250\n",
      " - 0s - loss: 0.6608 - acc: 0.7586 - val_loss: 0.6586 - val_acc: 0.7692\n",
      "Epoch 34/250\n",
      " - 0s - loss: 0.6654 - acc: 0.7586 - val_loss: 0.6573 - val_acc: 0.7692\n",
      "Epoch 35/250\n",
      " - 0s - loss: 0.6626 - acc: 0.7586 - val_loss: 0.6559 - val_acc: 0.7692\n",
      "Epoch 36/250\n",
      " - 0s - loss: 0.6572 - acc: 0.7586 - val_loss: 0.6544 - val_acc: 0.7692\n",
      "Epoch 37/250\n",
      " - 0s - loss: 0.6557 - acc: 0.7586 - val_loss: 0.6529 - val_acc: 0.7692\n",
      "Epoch 38/250\n",
      " - 0s - loss: 0.6612 - acc: 0.7586 - val_loss: 0.6511 - val_acc: 0.7692\n",
      "Epoch 39/250\n",
      " - 0s - loss: 0.6543 - acc: 0.7586 - val_loss: 0.6494 - val_acc: 0.7692\n",
      "Epoch 40/250\n",
      " - 0s - loss: 0.6561 - acc: 0.7586 - val_loss: 0.6480 - val_acc: 0.7692\n",
      "Epoch 41/250\n",
      " - 0s - loss: 0.6553 - acc: 0.7586 - val_loss: 0.6466 - val_acc: 0.7692\n",
      "Epoch 42/250\n",
      " - 0s - loss: 0.6623 - acc: 0.7586 - val_loss: 0.6455 - val_acc: 0.7692\n",
      "Epoch 43/250\n",
      " - 0s - loss: 0.6496 - acc: 0.7586 - val_loss: 0.6444 - val_acc: 0.7692\n",
      "Epoch 44/250\n",
      " - 0s - loss: 0.6466 - acc: 0.7586 - val_loss: 0.6434 - val_acc: 0.7692\n",
      "Epoch 45/250\n",
      " - 0s - loss: 0.6540 - acc: 0.7586 - val_loss: 0.6422 - val_acc: 0.7692\n",
      "Epoch 46/250\n",
      " - 0s - loss: 0.6498 - acc: 0.7586 - val_loss: 0.6411 - val_acc: 0.7692\n",
      "Epoch 47/250\n",
      " - 0s - loss: 0.6413 - acc: 0.7586 - val_loss: 0.6400 - val_acc: 0.7692\n",
      "Epoch 48/250\n",
      " - 0s - loss: 0.6449 - acc: 0.7586 - val_loss: 0.6390 - val_acc: 0.7692\n",
      "Epoch 49/250\n",
      " - 0s - loss: 0.6444 - acc: 0.7586 - val_loss: 0.6379 - val_acc: 0.7692\n",
      "Epoch 50/250\n",
      " - 0s - loss: 0.6501 - acc: 0.7586 - val_loss: 0.6369 - val_acc: 0.7692\n",
      "Epoch 51/250\n",
      " - 0s - loss: 0.6407 - acc: 0.7586 - val_loss: 0.6357 - val_acc: 0.7692\n",
      "Epoch 52/250\n",
      " - 0s - loss: 0.6416 - acc: 0.7586 - val_loss: 0.6345 - val_acc: 0.7692\n",
      "Epoch 53/250\n",
      " - 0s - loss: 0.6385 - acc: 0.7586 - val_loss: 0.6332 - val_acc: 0.7692\n",
      "Epoch 54/250\n",
      " - 0s - loss: 0.6401 - acc: 0.7586 - val_loss: 0.6320 - val_acc: 0.7692\n",
      "Epoch 55/250\n",
      " - 0s - loss: 0.6327 - acc: 0.7586 - val_loss: 0.6308 - val_acc: 0.7692\n",
      "Epoch 56/250\n",
      " - 0s - loss: 0.6336 - acc: 0.7586 - val_loss: 0.6298 - val_acc: 0.7692\n",
      "Epoch 57/250\n",
      " - 0s - loss: 0.6326 - acc: 0.7586 - val_loss: 0.6286 - val_acc: 0.7692\n",
      "Epoch 58/250\n",
      " - 0s - loss: 0.6306 - acc: 0.7586 - val_loss: 0.6273 - val_acc: 0.7692\n",
      "Epoch 59/250\n",
      " - 0s - loss: 0.6282 - acc: 0.7586 - val_loss: 0.6261 - val_acc: 0.7692\n",
      "Epoch 60/250\n",
      " - 0s - loss: 0.6321 - acc: 0.7586 - val_loss: 0.6248 - val_acc: 0.7692\n",
      "Epoch 61/250\n",
      " - 0s - loss: 0.6329 - acc: 0.7586 - val_loss: 0.6234 - val_acc: 0.7692\n",
      "Epoch 62/250\n",
      " - 0s - loss: 0.6254 - acc: 0.7586 - val_loss: 0.6221 - val_acc: 0.7692\n",
      "Epoch 63/250\n",
      " - 0s - loss: 0.6236 - acc: 0.7586 - val_loss: 0.6209 - val_acc: 0.7692\n",
      "Epoch 64/250\n",
      " - 0s - loss: 0.6241 - acc: 0.7586 - val_loss: 0.6196 - val_acc: 0.7692\n",
      "Epoch 65/250\n",
      " - 0s - loss: 0.6222 - acc: 0.7586 - val_loss: 0.6184 - val_acc: 0.7692\n",
      "Epoch 66/250\n",
      " - 0s - loss: 0.6278 - acc: 0.7586 - val_loss: 0.6172 - val_acc: 0.7692\n",
      "Epoch 67/250\n",
      " - 0s - loss: 0.6233 - acc: 0.7586 - val_loss: 0.6160 - val_acc: 0.7692\n",
      "Epoch 68/250\n",
      " - 0s - loss: 0.6176 - acc: 0.7586 - val_loss: 0.6148 - val_acc: 0.7692\n",
      "Epoch 69/250\n",
      " - 0s - loss: 0.6231 - acc: 0.7586 - val_loss: 0.6138 - val_acc: 0.7692\n",
      "Epoch 70/250\n",
      " - 0s - loss: 0.6092 - acc: 0.7586 - val_loss: 0.6126 - val_acc: 0.7692\n",
      "Epoch 71/250\n",
      " - 0s - loss: 0.6144 - acc: 0.7586 - val_loss: 0.6115 - val_acc: 0.7692\n",
      "Epoch 72/250\n",
      " - 0s - loss: 0.6148 - acc: 0.7586 - val_loss: 0.6104 - val_acc: 0.7692\n",
      "Epoch 73/250\n",
      " - 0s - loss: 0.6070 - acc: 0.7586 - val_loss: 0.6093 - val_acc: 0.7692\n",
      "Epoch 74/250\n",
      " - 0s - loss: 0.6121 - acc: 0.7586 - val_loss: 0.6082 - val_acc: 0.7692\n",
      "Epoch 75/250\n",
      " - 0s - loss: 0.6090 - acc: 0.7586 - val_loss: 0.6070 - val_acc: 0.7692\n",
      "Epoch 76/250\n",
      " - 0s - loss: 0.6105 - acc: 0.7586 - val_loss: 0.6060 - val_acc: 0.7692\n",
      "Epoch 77/250\n",
      " - 0s - loss: 0.6086 - acc: 0.7586 - val_loss: 0.6050 - val_acc: 0.7692\n",
      "Epoch 78/250\n",
      " - 0s - loss: 0.6147 - acc: 0.7586 - val_loss: 0.6038 - val_acc: 0.7692\n",
      "Epoch 79/250\n",
      " - 0s - loss: 0.6065 - acc: 0.7586 - val_loss: 0.6026 - val_acc: 0.7692\n",
      "Epoch 80/250\n",
      " - 0s - loss: 0.6067 - acc: 0.7586 - val_loss: 0.6013 - val_acc: 0.7692\n",
      "Epoch 81/250\n",
      " - 0s - loss: 0.6029 - acc: 0.7586 - val_loss: 0.6001 - val_acc: 0.7692\n",
      "Epoch 82/250\n",
      " - 0s - loss: 0.6011 - acc: 0.7586 - val_loss: 0.5989 - val_acc: 0.7692\n",
      "Epoch 83/250\n",
      " - 0s - loss: 0.5949 - acc: 0.7586 - val_loss: 0.5977 - val_acc: 0.7692\n",
      "Epoch 84/250\n",
      " - 0s - loss: 0.6038 - acc: 0.7586 - val_loss: 0.5964 - val_acc: 0.7692\n",
      "Epoch 85/250\n",
      " - 0s - loss: 0.5929 - acc: 0.7586 - val_loss: 0.5951 - val_acc: 0.7692\n",
      "Epoch 86/250\n",
      " - 0s - loss: 0.5884 - acc: 0.7586 - val_loss: 0.5937 - val_acc: 0.7692\n",
      "Epoch 87/250\n",
      " - 0s - loss: 0.5956 - acc: 0.7586 - val_loss: 0.5922 - val_acc: 0.7692\n",
      "Epoch 88/250\n",
      " - 0s - loss: 0.5862 - acc: 0.7586 - val_loss: 0.5907 - val_acc: 0.7692\n",
      "Epoch 89/250\n",
      " - 0s - loss: 0.5917 - acc: 0.7586 - val_loss: 0.5889 - val_acc: 0.7692\n",
      "Epoch 90/250\n",
      " - 0s - loss: 0.5860 - acc: 0.7586 - val_loss: 0.5872 - val_acc: 0.7692\n",
      "Epoch 91/250\n",
      " - 0s - loss: 0.5866 - acc: 0.7586 - val_loss: 0.5852 - val_acc: 0.7692\n",
      "Epoch 92/250\n",
      " - 0s - loss: 0.5812 - acc: 0.7586 - val_loss: 0.5832 - val_acc: 0.7692\n",
      "Epoch 93/250\n",
      " - 0s - loss: 0.5846 - acc: 0.7586 - val_loss: 0.5813 - val_acc: 0.7692\n",
      "Epoch 94/250\n",
      " - 0s - loss: 0.5737 - acc: 0.7586 - val_loss: 0.5792 - val_acc: 0.7692\n",
      "Epoch 95/250\n",
      " - 0s - loss: 0.5787 - acc: 0.7586 - val_loss: 0.5772 - val_acc: 0.7692\n",
      "Epoch 96/250\n",
      " - 0s - loss: 0.5656 - acc: 0.7586 - val_loss: 0.5749 - val_acc: 0.7692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/250\n",
      " - 0s - loss: 0.5800 - acc: 0.7586 - val_loss: 0.5726 - val_acc: 0.7692\n",
      "Epoch 98/250\n",
      " - 0s - loss: 0.5680 - acc: 0.7586 - val_loss: 0.5703 - val_acc: 0.7692\n",
      "Epoch 99/250\n",
      " - 0s - loss: 0.5695 - acc: 0.7586 - val_loss: 0.5682 - val_acc: 0.7692\n",
      "Epoch 100/250\n",
      " - 0s - loss: 0.5723 - acc: 0.7586 - val_loss: 0.5660 - val_acc: 0.7692\n",
      "Epoch 101/250\n",
      " - 0s - loss: 0.5562 - acc: 0.7586 - val_loss: 0.5638 - val_acc: 0.7692\n",
      "Epoch 102/250\n",
      " - 0s - loss: 0.5471 - acc: 0.7586 - val_loss: 0.5616 - val_acc: 0.7692\n",
      "Epoch 103/250\n",
      " - 0s - loss: 0.5676 - acc: 0.7586 - val_loss: 0.5593 - val_acc: 0.7692\n",
      "Epoch 104/250\n",
      " - 0s - loss: 0.5548 - acc: 0.7586 - val_loss: 0.5571 - val_acc: 0.7692\n",
      "Epoch 105/250\n",
      " - 0s - loss: 0.5382 - acc: 0.7586 - val_loss: 0.5549 - val_acc: 0.7692\n",
      "Epoch 106/250\n",
      " - 0s - loss: 0.5417 - acc: 0.7586 - val_loss: 0.5527 - val_acc: 0.7692\n",
      "Epoch 107/250\n",
      " - 0s - loss: 0.5617 - acc: 0.7586 - val_loss: 0.5507 - val_acc: 0.7692\n",
      "Epoch 108/250\n",
      " - 0s - loss: 0.5499 - acc: 0.7586 - val_loss: 0.5486 - val_acc: 0.7692\n",
      "Epoch 109/250\n",
      " - 0s - loss: 0.5438 - acc: 0.7586 - val_loss: 0.5468 - val_acc: 0.7692\n",
      "Epoch 110/250\n",
      " - 0s - loss: 0.5402 - acc: 0.7586 - val_loss: 0.5450 - val_acc: 0.7692\n",
      "Epoch 111/250\n",
      " - 0s - loss: 0.5378 - acc: 0.7586 - val_loss: 0.5431 - val_acc: 0.7692\n",
      "Epoch 112/250\n",
      " - 0s - loss: 0.5370 - acc: 0.7586 - val_loss: 0.5413 - val_acc: 0.7692\n",
      "Epoch 113/250\n",
      " - 0s - loss: 0.5275 - acc: 0.7586 - val_loss: 0.5393 - val_acc: 0.7692\n",
      "Epoch 114/250\n",
      " - 0s - loss: 0.5166 - acc: 0.7586 - val_loss: 0.5372 - val_acc: 0.7692\n",
      "Epoch 115/250\n",
      " - 0s - loss: 0.5376 - acc: 0.7586 - val_loss: 0.5351 - val_acc: 0.7692\n",
      "Epoch 116/250\n",
      " - 0s - loss: 0.5477 - acc: 0.7586 - val_loss: 0.5331 - val_acc: 0.7692\n",
      "Epoch 117/250\n",
      " - 0s - loss: 0.5076 - acc: 0.7586 - val_loss: 0.5312 - val_acc: 0.7692\n",
      "Epoch 118/250\n",
      " - 0s - loss: 0.5252 - acc: 0.7586 - val_loss: 0.5292 - val_acc: 0.7692\n",
      "Epoch 119/250\n",
      " - 0s - loss: 0.5378 - acc: 0.7586 - val_loss: 0.5274 - val_acc: 0.7692\n",
      "Epoch 120/250\n",
      " - 0s - loss: 0.5133 - acc: 0.7586 - val_loss: 0.5256 - val_acc: 0.7692\n",
      "Epoch 121/250\n",
      " - 0s - loss: 0.5125 - acc: 0.7586 - val_loss: 0.5239 - val_acc: 0.7692\n",
      "Epoch 122/250\n",
      " - 0s - loss: 0.5246 - acc: 0.7586 - val_loss: 0.5219 - val_acc: 0.7692\n",
      "Epoch 123/250\n",
      " - 0s - loss: 0.5422 - acc: 0.7586 - val_loss: 0.5200 - val_acc: 0.7692\n",
      "Epoch 124/250\n",
      " - 0s - loss: 0.5134 - acc: 0.7586 - val_loss: 0.5181 - val_acc: 0.7692\n",
      "Epoch 125/250\n",
      " - 0s - loss: 0.5078 - acc: 0.7586 - val_loss: 0.5161 - val_acc: 0.7692\n",
      "Epoch 126/250\n",
      " - 0s - loss: 0.5200 - acc: 0.7586 - val_loss: 0.5141 - val_acc: 0.7692\n",
      "Epoch 127/250\n",
      " - 0s - loss: 0.5093 - acc: 0.7586 - val_loss: 0.5121 - val_acc: 0.7692\n",
      "Epoch 128/250\n",
      " - 0s - loss: 0.5206 - acc: 0.7586 - val_loss: 0.5104 - val_acc: 0.7692\n",
      "Epoch 129/250\n",
      " - 0s - loss: 0.4635 - acc: 0.7586 - val_loss: 0.5085 - val_acc: 0.7692\n",
      "Epoch 130/250\n",
      " - 0s - loss: 0.4833 - acc: 0.7586 - val_loss: 0.5065 - val_acc: 0.7692\n",
      "Epoch 131/250\n",
      " - 0s - loss: 0.5142 - acc: 0.7586 - val_loss: 0.5045 - val_acc: 0.7692\n",
      "Epoch 132/250\n",
      " - 0s - loss: 0.5136 - acc: 0.7586 - val_loss: 0.5028 - val_acc: 0.7692\n",
      "Epoch 133/250\n",
      " - 0s - loss: 0.4565 - acc: 0.7586 - val_loss: 0.5011 - val_acc: 0.7692\n",
      "Epoch 134/250\n",
      " - 0s - loss: 0.4980 - acc: 0.7586 - val_loss: 0.4993 - val_acc: 0.7692\n",
      "Epoch 135/250\n",
      " - 0s - loss: 0.4778 - acc: 0.7586 - val_loss: 0.4975 - val_acc: 0.7692\n",
      "Epoch 136/250\n",
      " - 0s - loss: 0.4805 - acc: 0.7586 - val_loss: 0.4955 - val_acc: 0.7692\n",
      "Epoch 137/250\n",
      " - 0s - loss: 0.4690 - acc: 0.7586 - val_loss: 0.4934 - val_acc: 0.7692\n",
      "Epoch 138/250\n",
      " - 0s - loss: 0.4609 - acc: 0.7586 - val_loss: 0.4912 - val_acc: 0.7692\n",
      "Epoch 139/250\n",
      " - 0s - loss: 0.5034 - acc: 0.7586 - val_loss: 0.4889 - val_acc: 0.7692\n",
      "Epoch 140/250\n",
      " - 0s - loss: 0.4563 - acc: 0.7586 - val_loss: 0.4866 - val_acc: 0.7692\n",
      "Epoch 141/250\n",
      " - 0s - loss: 0.4810 - acc: 0.7586 - val_loss: 0.4842 - val_acc: 0.7692\n",
      "Epoch 142/250\n",
      " - 0s - loss: 0.4931 - acc: 0.7586 - val_loss: 0.4821 - val_acc: 0.7692\n",
      "Epoch 143/250\n",
      " - 0s - loss: 0.4790 - acc: 0.7586 - val_loss: 0.4800 - val_acc: 0.7692\n",
      "Epoch 144/250\n",
      " - 0s - loss: 0.4657 - acc: 0.7586 - val_loss: 0.4780 - val_acc: 0.7692\n",
      "Epoch 145/250\n",
      " - 0s - loss: 0.4827 - acc: 0.7586 - val_loss: 0.4762 - val_acc: 0.7692\n",
      "Epoch 146/250\n",
      " - 0s - loss: 0.4563 - acc: 0.7586 - val_loss: 0.4745 - val_acc: 0.7692\n",
      "Epoch 147/250\n",
      " - 0s - loss: 0.4259 - acc: 0.7586 - val_loss: 0.4726 - val_acc: 0.7692\n",
      "Epoch 148/250\n",
      " - 0s - loss: 0.4650 - acc: 0.7586 - val_loss: 0.4706 - val_acc: 0.7692\n",
      "Epoch 149/250\n",
      " - 0s - loss: 0.4678 - acc: 0.7586 - val_loss: 0.4688 - val_acc: 0.7692\n",
      "Epoch 150/250\n",
      " - 0s - loss: 0.4431 - acc: 0.7586 - val_loss: 0.4670 - val_acc: 0.7692\n",
      "Epoch 151/250\n",
      " - 0s - loss: 0.4429 - acc: 0.7586 - val_loss: 0.4653 - val_acc: 0.7692\n",
      "Epoch 152/250\n",
      " - 0s - loss: 0.4447 - acc: 0.7586 - val_loss: 0.4635 - val_acc: 0.7692\n",
      "Epoch 153/250\n",
      " - 0s - loss: 0.4424 - acc: 0.7586 - val_loss: 0.4619 - val_acc: 0.7692\n",
      "Epoch 154/250\n",
      " - 0s - loss: 0.4470 - acc: 0.7586 - val_loss: 0.4600 - val_acc: 0.7692\n",
      "Epoch 155/250\n",
      " - 0s - loss: 0.4841 - acc: 0.7586 - val_loss: 0.4581 - val_acc: 0.7692\n",
      "Epoch 156/250\n",
      " - 0s - loss: 0.4867 - acc: 0.7586 - val_loss: 0.4563 - val_acc: 0.7692\n",
      "Epoch 157/250\n",
      " - 0s - loss: 0.4968 - acc: 0.7586 - val_loss: 0.4548 - val_acc: 0.7692\n",
      "Epoch 158/250\n",
      " - 0s - loss: 0.4663 - acc: 0.7586 - val_loss: 0.4533 - val_acc: 0.7692\n",
      "Epoch 159/250\n",
      " - 0s - loss: 0.4358 - acc: 0.7586 - val_loss: 0.4518 - val_acc: 0.7692\n",
      "Epoch 160/250\n",
      " - 0s - loss: 0.4524 - acc: 0.7586 - val_loss: 0.4502 - val_acc: 0.7692\n",
      "Epoch 161/250\n",
      " - 0s - loss: 0.4447 - acc: 0.7586 - val_loss: 0.4487 - val_acc: 0.7692\n",
      "Epoch 162/250\n",
      " - 0s - loss: 0.4501 - acc: 0.7586 - val_loss: 0.4474 - val_acc: 0.7692\n",
      "Epoch 163/250\n",
      " - 0s - loss: 0.4565 - acc: 0.7586 - val_loss: 0.4464 - val_acc: 0.7692\n",
      "Epoch 164/250\n",
      " - 0s - loss: 0.4934 - acc: 0.7586 - val_loss: 0.4452 - val_acc: 0.7692\n",
      "Epoch 165/250\n",
      " - 0s - loss: 0.4664 - acc: 0.7586 - val_loss: 0.4442 - val_acc: 0.7692\n",
      "Epoch 166/250\n",
      " - 0s - loss: 0.4522 - acc: 0.7586 - val_loss: 0.4434 - val_acc: 0.7692\n",
      "Epoch 167/250\n",
      " - 0s - loss: 0.4388 - acc: 0.7586 - val_loss: 0.4423 - val_acc: 0.7692\n",
      "Epoch 168/250\n",
      " - 0s - loss: 0.4523 - acc: 0.7586 - val_loss: 0.4414 - val_acc: 0.7692\n",
      "Epoch 169/250\n",
      " - 0s - loss: 0.4976 - acc: 0.7586 - val_loss: 0.4403 - val_acc: 0.7692\n",
      "Epoch 170/250\n",
      " - 0s - loss: 0.4206 - acc: 0.7586 - val_loss: 0.4395 - val_acc: 0.7692\n",
      "Epoch 171/250\n",
      " - 0s - loss: 0.4637 - acc: 0.7586 - val_loss: 0.4384 - val_acc: 0.7692\n",
      "Epoch 172/250\n",
      " - 0s - loss: 0.4375 - acc: 0.7586 - val_loss: 0.4370 - val_acc: 0.7692\n",
      "Epoch 173/250\n",
      " - 0s - loss: 0.4090 - acc: 0.7586 - val_loss: 0.4356 - val_acc: 0.7692\n",
      "Epoch 174/250\n",
      " - 0s - loss: 0.4611 - acc: 0.7586 - val_loss: 0.4341 - val_acc: 0.7692\n",
      "Epoch 175/250\n",
      " - 0s - loss: 0.4289 - acc: 0.7586 - val_loss: 0.4325 - val_acc: 0.7692\n",
      "Epoch 176/250\n",
      " - 0s - loss: 0.4710 - acc: 0.7586 - val_loss: 0.4312 - val_acc: 0.7692\n",
      "Epoch 177/250\n",
      " - 0s - loss: 0.4058 - acc: 0.7586 - val_loss: 0.4299 - val_acc: 0.7692\n",
      "Epoch 178/250\n",
      " - 0s - loss: 0.4260 - acc: 0.7586 - val_loss: 0.4287 - val_acc: 0.7692\n",
      "Epoch 179/250\n",
      " - 0s - loss: 0.4203 - acc: 0.7586 - val_loss: 0.4277 - val_acc: 0.7692\n",
      "Epoch 180/250\n",
      " - 0s - loss: 0.4289 - acc: 0.7586 - val_loss: 0.4268 - val_acc: 0.7692\n",
      "Epoch 181/250\n",
      " - 0s - loss: 0.4207 - acc: 0.7586 - val_loss: 0.4253 - val_acc: 0.7692\n",
      "Epoch 182/250\n",
      " - 0s - loss: 0.4096 - acc: 0.7586 - val_loss: 0.4239 - val_acc: 0.7692\n",
      "Epoch 183/250\n",
      " - 0s - loss: 0.3660 - acc: 0.7586 - val_loss: 0.4230 - val_acc: 0.7692\n",
      "Epoch 184/250\n",
      " - 0s - loss: 0.4267 - acc: 0.7586 - val_loss: 0.4225 - val_acc: 0.7692\n",
      "Epoch 185/250\n",
      " - 0s - loss: 0.4338 - acc: 0.7586 - val_loss: 0.4218 - val_acc: 0.7692\n",
      "Epoch 186/250\n",
      " - 0s - loss: 0.3962 - acc: 0.7586 - val_loss: 0.4207 - val_acc: 0.7692\n",
      "Epoch 187/250\n",
      " - 0s - loss: 0.3713 - acc: 0.7586 - val_loss: 0.4198 - val_acc: 0.7692\n",
      "Epoch 188/250\n",
      " - 0s - loss: 0.4015 - acc: 0.7586 - val_loss: 0.4188 - val_acc: 0.7692\n",
      "Epoch 189/250\n",
      " - 0s - loss: 0.5107 - acc: 0.7586 - val_loss: 0.4180 - val_acc: 0.7692\n",
      "Epoch 190/250\n",
      " - 0s - loss: 0.4233 - acc: 0.7586 - val_loss: 0.4177 - val_acc: 0.7692\n",
      "Epoch 191/250\n",
      " - 0s - loss: 0.3545 - acc: 0.7586 - val_loss: 0.4174 - val_acc: 0.7692\n",
      "Epoch 192/250\n",
      " - 0s - loss: 0.3802 - acc: 0.7586 - val_loss: 0.4169 - val_acc: 0.7692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/250\n",
      " - 0s - loss: 0.3777 - acc: 0.7586 - val_loss: 0.4164 - val_acc: 0.7692\n",
      "Epoch 194/250\n",
      " - 0s - loss: 0.4376 - acc: 0.7586 - val_loss: 0.4159 - val_acc: 0.7692\n",
      "Epoch 195/250\n",
      " - 0s - loss: 0.3872 - acc: 0.7586 - val_loss: 0.4153 - val_acc: 0.7692\n",
      "Epoch 196/250\n",
      " - 0s - loss: 0.4069 - acc: 0.7586 - val_loss: 0.4145 - val_acc: 0.7692\n",
      "Epoch 197/250\n",
      " - 0s - loss: 0.4262 - acc: 0.7586 - val_loss: 0.4134 - val_acc: 0.7692\n",
      "Epoch 198/250\n",
      " - 0s - loss: 0.3640 - acc: 0.7586 - val_loss: 0.4128 - val_acc: 0.7692\n",
      "Epoch 199/250\n",
      " - 0s - loss: 0.3974 - acc: 0.7586 - val_loss: 0.4125 - val_acc: 0.7692\n",
      "Epoch 200/250\n",
      " - 0s - loss: 0.4538 - acc: 0.7586 - val_loss: 0.4117 - val_acc: 0.7692\n",
      "Epoch 201/250\n",
      " - 0s - loss: 0.4347 - acc: 0.7586 - val_loss: 0.4106 - val_acc: 0.7692\n",
      "Epoch 202/250\n",
      " - 0s - loss: 0.3908 - acc: 0.7586 - val_loss: 0.4092 - val_acc: 0.7692\n",
      "Epoch 203/250\n",
      " - 0s - loss: 0.4145 - acc: 0.7586 - val_loss: 0.4077 - val_acc: 0.7692\n",
      "Epoch 204/250\n",
      " - 0s - loss: 0.4041 - acc: 0.7586 - val_loss: 0.4065 - val_acc: 0.7692\n",
      "Epoch 205/250\n",
      " - 0s - loss: 0.4055 - acc: 0.7586 - val_loss: 0.4051 - val_acc: 0.7692\n",
      "Epoch 206/250\n",
      " - 0s - loss: 0.3854 - acc: 0.7586 - val_loss: 0.4037 - val_acc: 0.7692\n",
      "Epoch 207/250\n",
      " - 0s - loss: 0.4177 - acc: 0.7586 - val_loss: 0.4026 - val_acc: 0.7692\n",
      "Epoch 208/250\n",
      " - 0s - loss: 0.3864 - acc: 0.7586 - val_loss: 0.4012 - val_acc: 0.7692\n",
      "Epoch 209/250\n",
      " - 0s - loss: 0.3656 - acc: 0.7586 - val_loss: 0.3997 - val_acc: 0.7692\n",
      "Epoch 210/250\n",
      " - 0s - loss: 0.3665 - acc: 0.7586 - val_loss: 0.3986 - val_acc: 0.7692\n",
      "Epoch 211/250\n",
      " - 0s - loss: 0.3814 - acc: 0.7586 - val_loss: 0.3979 - val_acc: 0.7692\n",
      "Epoch 212/250\n",
      " - 0s - loss: 0.3699 - acc: 0.7586 - val_loss: 0.3969 - val_acc: 0.7692\n",
      "Epoch 213/250\n",
      " - 0s - loss: 0.3596 - acc: 0.7586 - val_loss: 0.3959 - val_acc: 0.7692\n",
      "Epoch 214/250\n",
      " - 0s - loss: 0.3631 - acc: 0.7586 - val_loss: 0.3954 - val_acc: 0.7692\n",
      "Epoch 215/250\n",
      " - 0s - loss: 0.4011 - acc: 0.7586 - val_loss: 0.3947 - val_acc: 0.7692\n",
      "Epoch 216/250\n",
      " - 0s - loss: 0.3653 - acc: 0.7586 - val_loss: 0.3941 - val_acc: 0.7692\n",
      "Epoch 217/250\n",
      " - 0s - loss: 0.4057 - acc: 0.7586 - val_loss: 0.3931 - val_acc: 0.7692\n",
      "Epoch 218/250\n",
      " - 0s - loss: 0.3739 - acc: 0.7586 - val_loss: 0.3921 - val_acc: 0.7692\n",
      "Epoch 219/250\n",
      " - 0s - loss: 0.3860 - acc: 0.7586 - val_loss: 0.3911 - val_acc: 0.7692\n",
      "Epoch 220/250\n",
      " - 0s - loss: 0.3849 - acc: 0.7586 - val_loss: 0.3897 - val_acc: 0.7692\n",
      "Epoch 221/250\n",
      " - 0s - loss: 0.3705 - acc: 0.7586 - val_loss: 0.3882 - val_acc: 0.7692\n",
      "Epoch 222/250\n",
      " - 0s - loss: 0.3786 - acc: 0.7586 - val_loss: 0.3860 - val_acc: 0.7692\n",
      "Epoch 223/250\n",
      " - 0s - loss: 0.3971 - acc: 0.7586 - val_loss: 0.3834 - val_acc: 0.7692\n",
      "Epoch 224/250\n",
      " - 0s - loss: 0.3613 - acc: 0.7586 - val_loss: 0.3803 - val_acc: 0.7692\n",
      "Epoch 225/250\n",
      " - 0s - loss: 0.3978 - acc: 0.7586 - val_loss: 0.3775 - val_acc: 0.7692\n",
      "Epoch 226/250\n",
      " - 0s - loss: 0.3878 - acc: 0.7586 - val_loss: 0.3757 - val_acc: 0.7692\n",
      "Epoch 227/250\n",
      " - 0s - loss: 0.3628 - acc: 0.7586 - val_loss: 0.3746 - val_acc: 0.7692\n",
      "Epoch 228/250\n",
      " - 0s - loss: 0.3465 - acc: 0.7586 - val_loss: 0.3742 - val_acc: 0.7692\n",
      "Epoch 229/250\n",
      " - 0s - loss: 0.3475 - acc: 0.7586 - val_loss: 0.3738 - val_acc: 0.7692\n",
      "Epoch 230/250\n",
      " - 0s - loss: 0.3666 - acc: 0.7586 - val_loss: 0.3733 - val_acc: 0.7692\n",
      "Epoch 231/250\n",
      " - 0s - loss: 0.3959 - acc: 0.7586 - val_loss: 0.3714 - val_acc: 0.7692\n",
      "Epoch 232/250\n",
      " - 0s - loss: 0.3280 - acc: 0.7586 - val_loss: 0.3697 - val_acc: 0.7692\n",
      "Epoch 233/250\n",
      " - 0s - loss: 0.3638 - acc: 0.7586 - val_loss: 0.3680 - val_acc: 0.7692\n",
      "Epoch 234/250\n",
      " - 0s - loss: 0.3430 - acc: 0.7586 - val_loss: 0.3668 - val_acc: 0.7692\n",
      "Epoch 235/250\n",
      " - 0s - loss: 0.3860 - acc: 0.7586 - val_loss: 0.3659 - val_acc: 0.7692\n",
      "Epoch 236/250\n",
      " - 0s - loss: 0.3658 - acc: 0.7586 - val_loss: 0.3649 - val_acc: 0.7692\n",
      "Epoch 237/250\n",
      " - 0s - loss: 0.3492 - acc: 0.7586 - val_loss: 0.3647 - val_acc: 0.7692\n",
      "Epoch 238/250\n",
      " - 0s - loss: 0.4181 - acc: 0.7586 - val_loss: 0.3648 - val_acc: 0.7692\n",
      "Epoch 239/250\n",
      " - 0s - loss: 0.3783 - acc: 0.7586 - val_loss: 0.3649 - val_acc: 0.7692\n",
      "Epoch 240/250\n",
      " - 0s - loss: 0.4314 - acc: 0.7586 - val_loss: 0.3649 - val_acc: 0.7692\n",
      "Epoch 241/250\n",
      " - 0s - loss: 0.3156 - acc: 0.7586 - val_loss: 0.3650 - val_acc: 0.7692\n",
      "Epoch 242/250\n",
      " - 0s - loss: 0.3757 - acc: 0.7586 - val_loss: 0.3651 - val_acc: 0.7692\n",
      "Epoch 243/250\n",
      " - 0s - loss: 0.3730 - acc: 0.7586 - val_loss: 0.3654 - val_acc: 0.7692\n",
      "Epoch 244/250\n",
      " - 0s - loss: 0.4146 - acc: 0.7586 - val_loss: 0.3655 - val_acc: 0.7692\n",
      "Epoch 245/250\n",
      " - 0s - loss: 0.3406 - acc: 0.7586 - val_loss: 0.3655 - val_acc: 0.7692\n",
      "Epoch 246/250\n",
      " - 0s - loss: 0.3613 - acc: 0.7586 - val_loss: 0.3656 - val_acc: 0.7692\n",
      "Epoch 247/250\n",
      " - 0s - loss: 0.3186 - acc: 0.7586 - val_loss: 0.3656 - val_acc: 0.7692\n",
      "Epoch 248/250\n",
      " - 0s - loss: 0.3804 - acc: 0.7586 - val_loss: 0.3654 - val_acc: 0.7692\n",
      "Epoch 249/250\n",
      " - 0s - loss: 0.3606 - acc: 0.7586 - val_loss: 0.3660 - val_acc: 0.7692\n",
      "Epoch 250/250\n",
      " - 0s - loss: 0.3841 - acc: 0.7586 - val_loss: 0.3659 - val_acc: 0.7692\n"
     ]
    }
   ],
   "source": [
    "mlp_search = mlp_search(shape)\n",
    "history = mlp_search.fit(X_train, np.asarray(y_train[\"Search\"]).reshape(-1,1),\n",
    "                  validation_data=(X_validation, np.asarray(y_validation[\"Search\"]).reshape(-1,1)),\n",
    "    epochs=250,\n",
    "    workers = 2, use_multiprocessing= True, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_suggestion(shape):\n",
    "# define our MLP network\n",
    "    initializer = tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5, seed=42)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=shape, kernel_initializer = initializer, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(2, activation=\"relu\"))\n",
    "# check to see if the regression node should be added\n",
    "    #if regress:\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    #Compile model\n",
    "    opt = tf.keras.optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss='binary_crossentropy', metrics = [\"accuracy\"], optimizer = opt)\n",
    "# return our model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58 samples, validate on 26 samples\n",
      "Epoch 1/250\n",
      " - 1s - loss: 0.6945 - acc: 0.5517 - val_loss: 0.6888 - val_acc: 0.6923\n",
      "Epoch 2/250\n",
      " - 0s - loss: 0.6931 - acc: 0.5517 - val_loss: 0.6870 - val_acc: 0.7308\n",
      "Epoch 3/250\n",
      " - 0s - loss: 0.6838 - acc: 0.6897 - val_loss: 0.6848 - val_acc: 0.7308\n",
      "Epoch 4/250\n",
      " - 0s - loss: 0.6746 - acc: 0.7241 - val_loss: 0.6825 - val_acc: 0.7308\n",
      "Epoch 5/250\n",
      " - 0s - loss: 0.6917 - acc: 0.6034 - val_loss: 0.6800 - val_acc: 0.7308\n",
      "Epoch 6/250\n",
      " - 0s - loss: 0.6746 - acc: 0.7069 - val_loss: 0.6773 - val_acc: 0.7308\n",
      "Epoch 7/250\n",
      " - 0s - loss: 0.6993 - acc: 0.7241 - val_loss: 0.6750 - val_acc: 0.7308\n",
      "Epoch 8/250\n",
      " - 0s - loss: 0.6914 - acc: 0.7241 - val_loss: 0.6725 - val_acc: 0.7692\n",
      "Epoch 9/250\n",
      " - 0s - loss: 0.6812 - acc: 0.6724 - val_loss: 0.6699 - val_acc: 0.7692\n",
      "Epoch 10/250\n",
      " - 0s - loss: 0.6753 - acc: 0.7414 - val_loss: 0.6673 - val_acc: 0.7692\n",
      "Epoch 11/250\n",
      " - 0s - loss: 0.6563 - acc: 0.7414 - val_loss: 0.6644 - val_acc: 0.7692\n",
      "Epoch 12/250\n",
      " - 0s - loss: 0.6519 - acc: 0.7069 - val_loss: 0.6614 - val_acc: 0.7692\n",
      "Epoch 13/250\n",
      " - 0s - loss: 0.6784 - acc: 0.7069 - val_loss: 0.6583 - val_acc: 0.7692\n",
      "Epoch 14/250\n",
      " - 0s - loss: 0.6535 - acc: 0.7586 - val_loss: 0.6551 - val_acc: 0.7692\n",
      "Epoch 15/250\n",
      " - 0s - loss: 0.6488 - acc: 0.7759 - val_loss: 0.6517 - val_acc: 0.7692\n",
      "Epoch 16/250\n",
      " - 0s - loss: 0.6428 - acc: 0.7586 - val_loss: 0.6484 - val_acc: 0.7692\n",
      "Epoch 17/250\n",
      " - 0s - loss: 0.6479 - acc: 0.7414 - val_loss: 0.6450 - val_acc: 0.7692\n",
      "Epoch 18/250\n",
      " - 0s - loss: 0.6294 - acc: 0.7759 - val_loss: 0.6418 - val_acc: 0.7692\n",
      "Epoch 19/250\n",
      " - 0s - loss: 0.6312 - acc: 0.7586 - val_loss: 0.6385 - val_acc: 0.7692\n",
      "Epoch 20/250\n",
      " - 0s - loss: 0.6367 - acc: 0.7586 - val_loss: 0.6348 - val_acc: 0.7692\n",
      "Epoch 21/250\n",
      " - 0s - loss: 0.6615 - acc: 0.7414 - val_loss: 0.6309 - val_acc: 0.7692\n",
      "Epoch 22/250\n",
      " - 0s - loss: 0.6622 - acc: 0.7414 - val_loss: 0.6271 - val_acc: 0.7692\n",
      "Epoch 23/250\n",
      " - 0s - loss: 0.6355 - acc: 0.7586 - val_loss: 0.6233 - val_acc: 0.7692\n",
      "Epoch 24/250\n",
      " - 0s - loss: 0.6313 - acc: 0.7586 - val_loss: 0.6191 - val_acc: 0.7692\n",
      "Epoch 25/250\n",
      " - 0s - loss: 0.6274 - acc: 0.7586 - val_loss: 0.6150 - val_acc: 0.7692\n",
      "Epoch 26/250\n",
      " - 0s - loss: 0.6322 - acc: 0.7586 - val_loss: 0.6109 - val_acc: 0.7692\n",
      "Epoch 27/250\n",
      " - 0s - loss: 0.6001 - acc: 0.7586 - val_loss: 0.6063 - val_acc: 0.7692\n",
      "Epoch 28/250\n",
      " - 0s - loss: 0.6010 - acc: 0.7586 - val_loss: 0.6017 - val_acc: 0.7692\n",
      "Epoch 29/250\n",
      " - 0s - loss: 0.6102 - acc: 0.7586 - val_loss: 0.5971 - val_acc: 0.7692\n",
      "Epoch 30/250\n",
      " - 0s - loss: 0.6157 - acc: 0.7586 - val_loss: 0.5926 - val_acc: 0.7692\n",
      "Epoch 31/250\n",
      " - 0s - loss: 0.6076 - acc: 0.7586 - val_loss: 0.5883 - val_acc: 0.7692\n",
      "Epoch 32/250\n",
      " - 0s - loss: 0.5972 - acc: 0.7586 - val_loss: 0.5841 - val_acc: 0.7692\n",
      "Epoch 33/250\n",
      " - 0s - loss: 0.5965 - acc: 0.7414 - val_loss: 0.5801 - val_acc: 0.7692\n",
      "Epoch 34/250\n",
      " - 0s - loss: 0.5985 - acc: 0.7586 - val_loss: 0.5762 - val_acc: 0.7692\n",
      "Epoch 35/250\n",
      " - 0s - loss: 0.6024 - acc: 0.7586 - val_loss: 0.5724 - val_acc: 0.7692\n",
      "Epoch 36/250\n",
      " - 0s - loss: 0.5940 - acc: 0.7586 - val_loss: 0.5686 - val_acc: 0.7692\n",
      "Epoch 37/250\n",
      " - 0s - loss: 0.6115 - acc: 0.7586 - val_loss: 0.5650 - val_acc: 0.7692\n",
      "Epoch 38/250\n",
      " - 0s - loss: 0.5720 - acc: 0.7586 - val_loss: 0.5617 - val_acc: 0.7692\n",
      "Epoch 39/250\n",
      " - 0s - loss: 0.5501 - acc: 0.7586 - val_loss: 0.5581 - val_acc: 0.7692\n",
      "Epoch 40/250\n",
      " - 0s - loss: 0.5775 - acc: 0.7586 - val_loss: 0.5547 - val_acc: 0.7692\n",
      "Epoch 41/250\n",
      " - 0s - loss: 0.5530 - acc: 0.7586 - val_loss: 0.5511 - val_acc: 0.7692\n",
      "Epoch 42/250\n",
      " - 0s - loss: 0.5548 - acc: 0.7586 - val_loss: 0.5476 - val_acc: 0.7692\n",
      "Epoch 43/250\n",
      " - 0s - loss: 0.5423 - acc: 0.7586 - val_loss: 0.5444 - val_acc: 0.7692\n",
      "Epoch 44/250\n",
      " - 0s - loss: 0.5237 - acc: 0.7586 - val_loss: 0.5412 - val_acc: 0.7692\n",
      "Epoch 45/250\n",
      " - 0s - loss: 0.5358 - acc: 0.7586 - val_loss: 0.5378 - val_acc: 0.7692\n",
      "Epoch 46/250\n",
      " - 0s - loss: 0.5468 - acc: 0.7586 - val_loss: 0.5348 - val_acc: 0.7692\n",
      "Epoch 47/250\n",
      " - 0s - loss: 0.5524 - acc: 0.7586 - val_loss: 0.5320 - val_acc: 0.7692\n",
      "Epoch 48/250\n",
      " - 0s - loss: 0.5162 - acc: 0.7586 - val_loss: 0.5292 - val_acc: 0.7692\n",
      "Epoch 49/250\n",
      " - 0s - loss: 0.5475 - acc: 0.7586 - val_loss: 0.5267 - val_acc: 0.7692\n",
      "Epoch 50/250\n",
      " - 0s - loss: 0.5735 - acc: 0.7586 - val_loss: 0.5245 - val_acc: 0.7692\n",
      "Epoch 51/250\n",
      " - 0s - loss: 0.5438 - acc: 0.7586 - val_loss: 0.5223 - val_acc: 0.7692\n",
      "Epoch 52/250\n",
      " - 0s - loss: 0.5430 - acc: 0.7586 - val_loss: 0.5201 - val_acc: 0.7692\n",
      "Epoch 53/250\n",
      " - 0s - loss: 0.5310 - acc: 0.7586 - val_loss: 0.5179 - val_acc: 0.7692\n",
      "Epoch 54/250\n",
      " - 0s - loss: 0.5442 - acc: 0.7586 - val_loss: 0.5157 - val_acc: 0.7692\n",
      "Epoch 55/250\n",
      " - 0s - loss: 0.5502 - acc: 0.7586 - val_loss: 0.5135 - val_acc: 0.7692\n",
      "Epoch 56/250\n",
      " - 0s - loss: 0.5149 - acc: 0.7586 - val_loss: 0.5112 - val_acc: 0.7692\n",
      "Epoch 57/250\n",
      " - 0s - loss: 0.5534 - acc: 0.7586 - val_loss: 0.5089 - val_acc: 0.7692\n",
      "Epoch 58/250\n",
      " - 0s - loss: 0.4949 - acc: 0.7586 - val_loss: 0.5067 - val_acc: 0.7692\n",
      "Epoch 59/250\n",
      " - 0s - loss: 0.5025 - acc: 0.7586 - val_loss: 0.5045 - val_acc: 0.7692\n",
      "Epoch 60/250\n",
      " - 0s - loss: 0.5078 - acc: 0.7586 - val_loss: 0.5025 - val_acc: 0.7692\n",
      "Epoch 61/250\n",
      " - 0s - loss: 0.4979 - acc: 0.7586 - val_loss: 0.5005 - val_acc: 0.7692\n",
      "Epoch 62/250\n",
      " - 0s - loss: 0.5162 - acc: 0.7586 - val_loss: 0.4984 - val_acc: 0.7692\n",
      "Epoch 63/250\n",
      " - 0s - loss: 0.4916 - acc: 0.7586 - val_loss: 0.4964 - val_acc: 0.7692\n",
      "Epoch 64/250\n",
      " - 0s - loss: 0.5220 - acc: 0.7586 - val_loss: 0.4944 - val_acc: 0.7692\n",
      "Epoch 65/250\n",
      " - 0s - loss: 0.5063 - acc: 0.7586 - val_loss: 0.4925 - val_acc: 0.7692\n",
      "Epoch 66/250\n",
      " - 0s - loss: 0.5166 - acc: 0.7586 - val_loss: 0.4906 - val_acc: 0.7692\n",
      "Epoch 67/250\n",
      " - 0s - loss: 0.5325 - acc: 0.7586 - val_loss: 0.4888 - val_acc: 0.7692\n",
      "Epoch 68/250\n",
      " - 0s - loss: 0.4482 - acc: 0.7586 - val_loss: 0.4869 - val_acc: 0.7692\n",
      "Epoch 69/250\n",
      " - 0s - loss: 0.4522 - acc: 0.7586 - val_loss: 0.4849 - val_acc: 0.7692\n",
      "Epoch 70/250\n",
      " - 0s - loss: 0.4871 - acc: 0.7586 - val_loss: 0.4830 - val_acc: 0.7692\n",
      "Epoch 71/250\n",
      " - 0s - loss: 0.4395 - acc: 0.7586 - val_loss: 0.4812 - val_acc: 0.7692\n",
      "Epoch 72/250\n",
      " - 0s - loss: 0.5032 - acc: 0.7586 - val_loss: 0.4794 - val_acc: 0.7692\n",
      "Epoch 73/250\n",
      " - 0s - loss: 0.4458 - acc: 0.7586 - val_loss: 0.4776 - val_acc: 0.7692\n",
      "Epoch 74/250\n",
      " - 0s - loss: 0.4642 - acc: 0.7586 - val_loss: 0.4759 - val_acc: 0.7692\n",
      "Epoch 75/250\n",
      " - 0s - loss: 0.4703 - acc: 0.7586 - val_loss: 0.4744 - val_acc: 0.7692\n",
      "Epoch 76/250\n",
      " - 0s - loss: 0.4703 - acc: 0.7586 - val_loss: 0.4729 - val_acc: 0.7692\n",
      "Epoch 77/250\n",
      " - 0s - loss: 0.4315 - acc: 0.7586 - val_loss: 0.4715 - val_acc: 0.7692\n",
      "Epoch 78/250\n",
      " - 0s - loss: 0.4693 - acc: 0.7586 - val_loss: 0.4699 - val_acc: 0.7692\n",
      "Epoch 79/250\n",
      " - 0s - loss: 0.4148 - acc: 0.7586 - val_loss: 0.4684 - val_acc: 0.7692\n",
      "Epoch 80/250\n",
      " - 0s - loss: 0.4582 - acc: 0.7586 - val_loss: 0.4671 - val_acc: 0.7692\n",
      "Epoch 81/250\n",
      " - 0s - loss: 0.4257 - acc: 0.7586 - val_loss: 0.4658 - val_acc: 0.7692\n",
      "Epoch 82/250\n",
      " - 0s - loss: 0.4325 - acc: 0.7586 - val_loss: 0.4645 - val_acc: 0.7692\n",
      "Epoch 83/250\n",
      " - 0s - loss: 0.4522 - acc: 0.7586 - val_loss: 0.4631 - val_acc: 0.7692\n",
      "Epoch 84/250\n",
      " - 0s - loss: 0.4333 - acc: 0.7586 - val_loss: 0.4618 - val_acc: 0.7692\n",
      "Epoch 85/250\n",
      " - 0s - loss: 0.4200 - acc: 0.7586 - val_loss: 0.4603 - val_acc: 0.7692\n",
      "Epoch 86/250\n",
      " - 0s - loss: 0.4489 - acc: 0.7586 - val_loss: 0.4586 - val_acc: 0.7692\n",
      "Epoch 87/250\n",
      " - 0s - loss: 0.4491 - acc: 0.7586 - val_loss: 0.4569 - val_acc: 0.7692\n",
      "Epoch 88/250\n",
      " - 0s - loss: 0.4442 - acc: 0.7586 - val_loss: 0.4554 - val_acc: 0.7692\n",
      "Epoch 89/250\n",
      " - 0s - loss: 0.4405 - acc: 0.7586 - val_loss: 0.4540 - val_acc: 0.7692\n",
      "Epoch 90/250\n",
      " - 0s - loss: 0.3781 - acc: 0.7586 - val_loss: 0.4528 - val_acc: 0.7692\n",
      "Epoch 91/250\n",
      " - 0s - loss: 0.4125 - acc: 0.7586 - val_loss: 0.4518 - val_acc: 0.7692\n",
      "Epoch 92/250\n",
      " - 0s - loss: 0.4035 - acc: 0.7586 - val_loss: 0.4510 - val_acc: 0.7692\n",
      "Epoch 93/250\n",
      " - 0s - loss: 0.4498 - acc: 0.7586 - val_loss: 0.4503 - val_acc: 0.7692\n",
      "Epoch 94/250\n",
      " - 0s - loss: 0.4067 - acc: 0.7586 - val_loss: 0.4495 - val_acc: 0.7692\n",
      "Epoch 95/250\n",
      " - 0s - loss: 0.4285 - acc: 0.7586 - val_loss: 0.4488 - val_acc: 0.7692\n",
      "Epoch 96/250\n",
      " - 0s - loss: 0.4332 - acc: 0.7586 - val_loss: 0.4480 - val_acc: 0.7692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/250\n",
      " - 0s - loss: 0.4043 - acc: 0.7586 - val_loss: 0.4474 - val_acc: 0.7692\n",
      "Epoch 98/250\n",
      " - 0s - loss: 0.3824 - acc: 0.7586 - val_loss: 0.4468 - val_acc: 0.7692\n",
      "Epoch 99/250\n",
      " - 0s - loss: 0.4581 - acc: 0.7586 - val_loss: 0.4458 - val_acc: 0.7692\n",
      "Epoch 100/250\n",
      " - 0s - loss: 0.4018 - acc: 0.7586 - val_loss: 0.4446 - val_acc: 0.7692\n",
      "Epoch 101/250\n",
      " - 0s - loss: 0.4158 - acc: 0.7586 - val_loss: 0.4434 - val_acc: 0.7692\n",
      "Epoch 102/250\n",
      " - 0s - loss: 0.3949 - acc: 0.7586 - val_loss: 0.4424 - val_acc: 0.7692\n",
      "Epoch 103/250\n",
      " - 0s - loss: 0.3888 - acc: 0.7586 - val_loss: 0.4418 - val_acc: 0.7692\n",
      "Epoch 104/250\n",
      " - 0s - loss: 0.3864 - acc: 0.7586 - val_loss: 0.4414 - val_acc: 0.7692\n",
      "Epoch 105/250\n",
      " - 0s - loss: 0.3937 - acc: 0.7586 - val_loss: 0.4411 - val_acc: 0.7692\n",
      "Epoch 106/250\n",
      " - 0s - loss: 0.4446 - acc: 0.7586 - val_loss: 0.4404 - val_acc: 0.7692\n",
      "Epoch 107/250\n",
      " - 0s - loss: 0.3918 - acc: 0.7586 - val_loss: 0.4402 - val_acc: 0.7692\n",
      "Epoch 108/250\n",
      " - 0s - loss: 0.3894 - acc: 0.7586 - val_loss: 0.4402 - val_acc: 0.7692\n",
      "Epoch 109/250\n",
      " - 0s - loss: 0.4334 - acc: 0.7586 - val_loss: 0.4401 - val_acc: 0.7692\n",
      "Epoch 110/250\n",
      " - 0s - loss: 0.3955 - acc: 0.7586 - val_loss: 0.4402 - val_acc: 0.7692\n",
      "Epoch 111/250\n",
      " - 0s - loss: 0.3517 - acc: 0.7586 - val_loss: 0.4404 - val_acc: 0.7692\n",
      "Epoch 112/250\n",
      " - 0s - loss: 0.3794 - acc: 0.7586 - val_loss: 0.4408 - val_acc: 0.7692\n",
      "Epoch 113/250\n",
      " - 0s - loss: 0.4178 - acc: 0.7586 - val_loss: 0.4411 - val_acc: 0.7692\n",
      "Epoch 114/250\n",
      " - 0s - loss: 0.4206 - acc: 0.7586 - val_loss: 0.4415 - val_acc: 0.7692\n",
      "Epoch 115/250\n",
      " - 0s - loss: 0.4062 - acc: 0.7586 - val_loss: 0.4420 - val_acc: 0.7692\n",
      "Epoch 116/250\n",
      " - 0s - loss: 0.3827 - acc: 0.7586 - val_loss: 0.4422 - val_acc: 0.7692\n",
      "Epoch 117/250\n",
      " - 0s - loss: 0.3921 - acc: 0.7586 - val_loss: 0.4424 - val_acc: 0.7692\n",
      "Epoch 118/250\n",
      " - 0s - loss: 0.3149 - acc: 0.7586 - val_loss: 0.4427 - val_acc: 0.7692\n",
      "Epoch 119/250\n",
      " - 0s - loss: 0.4056 - acc: 0.7586 - val_loss: 0.4433 - val_acc: 0.7692\n",
      "Epoch 120/250\n",
      " - 0s - loss: 0.3794 - acc: 0.7586 - val_loss: 0.4438 - val_acc: 0.7692\n",
      "Epoch 121/250\n",
      " - 0s - loss: 0.3792 - acc: 0.7586 - val_loss: 0.4439 - val_acc: 0.7692\n",
      "Epoch 122/250\n",
      " - 0s - loss: 0.4021 - acc: 0.7586 - val_loss: 0.4443 - val_acc: 0.7692\n",
      "Epoch 123/250\n",
      " - 0s - loss: 0.3435 - acc: 0.7586 - val_loss: 0.4447 - val_acc: 0.7692\n",
      "Epoch 124/250\n",
      " - 0s - loss: 0.3440 - acc: 0.7586 - val_loss: 0.4449 - val_acc: 0.7692\n",
      "Epoch 125/250\n",
      " - 0s - loss: 0.3774 - acc: 0.7586 - val_loss: 0.4450 - val_acc: 0.7692\n",
      "Epoch 126/250\n",
      " - 0s - loss: 0.3241 - acc: 0.7586 - val_loss: 0.4451 - val_acc: 0.7692\n",
      "Epoch 127/250\n",
      " - 0s - loss: 0.3294 - acc: 0.7586 - val_loss: 0.4457 - val_acc: 0.7692\n",
      "Epoch 128/250\n",
      " - 0s - loss: 0.3588 - acc: 0.7586 - val_loss: 0.4459 - val_acc: 0.7692\n",
      "Epoch 129/250\n",
      " - 0s - loss: 0.3687 - acc: 0.7586 - val_loss: 0.4471 - val_acc: 0.7692\n",
      "Epoch 130/250\n",
      " - 0s - loss: 0.3571 - acc: 0.7586 - val_loss: 0.4489 - val_acc: 0.7692\n",
      "Epoch 131/250\n",
      " - 0s - loss: 0.3281 - acc: 0.7586 - val_loss: 0.4505 - val_acc: 0.7692\n",
      "Epoch 132/250\n",
      " - 0s - loss: 0.3146 - acc: 0.7586 - val_loss: 0.4517 - val_acc: 0.7692\n",
      "Epoch 133/250\n",
      " - 0s - loss: 0.3346 - acc: 0.7586 - val_loss: 0.4530 - val_acc: 0.7692\n",
      "Epoch 134/250\n",
      " - 0s - loss: 0.3179 - acc: 0.7586 - val_loss: 0.4543 - val_acc: 0.7692\n",
      "Epoch 135/250\n",
      " - 0s - loss: 0.3878 - acc: 0.7586 - val_loss: 0.4548 - val_acc: 0.7692\n",
      "Epoch 136/250\n",
      " - 0s - loss: 0.3752 - acc: 0.7586 - val_loss: 0.4547 - val_acc: 0.7692\n",
      "Epoch 137/250\n",
      " - 0s - loss: 0.3101 - acc: 0.7586 - val_loss: 0.4553 - val_acc: 0.7692\n",
      "Epoch 138/250\n",
      " - 0s - loss: 0.3740 - acc: 0.7586 - val_loss: 0.4552 - val_acc: 0.7692\n",
      "Epoch 139/250\n",
      " - 0s - loss: 0.3575 - acc: 0.7586 - val_loss: 0.4543 - val_acc: 0.7692\n",
      "Epoch 140/250\n",
      " - 0s - loss: 0.3262 - acc: 0.7586 - val_loss: 0.4543 - val_acc: 0.7692\n",
      "Epoch 141/250\n",
      " - 0s - loss: 0.2995 - acc: 0.7586 - val_loss: 0.4551 - val_acc: 0.7692\n",
      "Epoch 142/250\n",
      " - 0s - loss: 0.3680 - acc: 0.7586 - val_loss: 0.4562 - val_acc: 0.7692\n",
      "Epoch 143/250\n",
      " - 0s - loss: 0.3122 - acc: 0.7586 - val_loss: 0.4577 - val_acc: 0.7692\n",
      "Epoch 144/250\n",
      " - 0s - loss: 0.3168 - acc: 0.7586 - val_loss: 0.4598 - val_acc: 0.7692\n",
      "Epoch 145/250\n",
      " - 0s - loss: 0.3547 - acc: 0.7586 - val_loss: 0.4618 - val_acc: 0.7692\n",
      "Epoch 146/250\n",
      " - 0s - loss: 0.3139 - acc: 0.7586 - val_loss: 0.4632 - val_acc: 0.7692\n",
      "Epoch 147/250\n",
      " - 0s - loss: 0.2965 - acc: 0.7586 - val_loss: 0.4653 - val_acc: 0.7692\n",
      "Epoch 148/250\n",
      " - 0s - loss: 0.3125 - acc: 0.7586 - val_loss: 0.4682 - val_acc: 0.7692\n",
      "Epoch 149/250\n",
      " - 0s - loss: 0.3234 - acc: 0.7586 - val_loss: 0.4715 - val_acc: 0.7692\n",
      "Epoch 150/250\n",
      " - 0s - loss: 0.3214 - acc: 0.7586 - val_loss: 0.4743 - val_acc: 0.7692\n",
      "Epoch 151/250\n",
      " - 0s - loss: 0.3359 - acc: 0.7586 - val_loss: 0.4763 - val_acc: 0.7692\n",
      "Epoch 152/250\n",
      " - 0s - loss: 0.2886 - acc: 0.7586 - val_loss: 0.4776 - val_acc: 0.7692\n",
      "Epoch 153/250\n",
      " - 0s - loss: 0.2982 - acc: 0.7586 - val_loss: 0.4791 - val_acc: 0.7692\n",
      "Epoch 154/250\n",
      " - 0s - loss: 0.3594 - acc: 0.7586 - val_loss: 0.4783 - val_acc: 0.7692\n",
      "Epoch 155/250\n",
      " - 0s - loss: 0.3420 - acc: 0.7586 - val_loss: 0.4768 - val_acc: 0.7692\n",
      "Epoch 156/250\n",
      " - 0s - loss: 0.3230 - acc: 0.7586 - val_loss: 0.4746 - val_acc: 0.7692\n",
      "Epoch 157/250\n",
      " - 0s - loss: 0.3535 - acc: 0.7586 - val_loss: 0.4727 - val_acc: 0.7692\n",
      "Epoch 158/250\n",
      " - 0s - loss: 0.3018 - acc: 0.7586 - val_loss: 0.4717 - val_acc: 0.7692\n",
      "Epoch 159/250\n",
      " - 0s - loss: 0.3072 - acc: 0.7586 - val_loss: 0.4706 - val_acc: 0.7692\n",
      "Epoch 160/250\n",
      " - 0s - loss: 0.3345 - acc: 0.7586 - val_loss: 0.4697 - val_acc: 0.7692\n",
      "Epoch 161/250\n",
      " - 0s - loss: 0.3326 - acc: 0.7586 - val_loss: 0.4694 - val_acc: 0.7692\n",
      "Epoch 162/250\n",
      " - 0s - loss: 0.3593 - acc: 0.7586 - val_loss: 0.4690 - val_acc: 0.7692\n",
      "Epoch 163/250\n",
      " - 0s - loss: 0.3044 - acc: 0.7586 - val_loss: 0.4689 - val_acc: 0.7692\n",
      "Epoch 164/250\n",
      " - 0s - loss: 0.2982 - acc: 0.7586 - val_loss: 0.4695 - val_acc: 0.7692\n",
      "Epoch 165/250\n",
      " - 0s - loss: 0.3212 - acc: 0.7586 - val_loss: 0.4698 - val_acc: 0.7692\n",
      "Epoch 166/250\n",
      " - 0s - loss: 0.2838 - acc: 0.7586 - val_loss: 0.4705 - val_acc: 0.7692\n",
      "Epoch 167/250\n",
      " - 0s - loss: 0.2951 - acc: 0.7586 - val_loss: 0.4716 - val_acc: 0.7692\n",
      "Epoch 168/250\n",
      " - 0s - loss: 0.2821 - acc: 0.7586 - val_loss: 0.4733 - val_acc: 0.7692\n",
      "Epoch 169/250\n",
      " - 0s - loss: 0.3065 - acc: 0.7586 - val_loss: 0.4753 - val_acc: 0.7692\n",
      "Epoch 170/250\n",
      " - 0s - loss: 0.3208 - acc: 0.7586 - val_loss: 0.4772 - val_acc: 0.7692\n",
      "Epoch 171/250\n",
      " - 0s - loss: 0.3012 - acc: 0.7586 - val_loss: 0.4786 - val_acc: 0.7692\n",
      "Epoch 172/250\n",
      " - 0s - loss: 0.2816 - acc: 0.7586 - val_loss: 0.4793 - val_acc: 0.7692\n",
      "Epoch 173/250\n",
      " - 0s - loss: 0.3442 - acc: 0.7586 - val_loss: 0.4793 - val_acc: 0.7692\n",
      "Epoch 174/250\n",
      " - 0s - loss: 0.3024 - acc: 0.7586 - val_loss: 0.4793 - val_acc: 0.7692\n",
      "Epoch 175/250\n",
      " - 0s - loss: 0.2774 - acc: 0.7586 - val_loss: 0.4796 - val_acc: 0.7692\n",
      "Epoch 176/250\n",
      " - 0s - loss: 0.2877 - acc: 0.7586 - val_loss: 0.4804 - val_acc: 0.7692\n",
      "Epoch 177/250\n",
      " - 0s - loss: 0.3586 - acc: 0.7586 - val_loss: 0.4817 - val_acc: 0.7692\n",
      "Epoch 178/250\n",
      " - 0s - loss: 0.2838 - acc: 0.7586 - val_loss: 0.4828 - val_acc: 0.7692\n",
      "Epoch 179/250\n",
      " - 0s - loss: 0.3046 - acc: 0.7586 - val_loss: 0.4846 - val_acc: 0.7692\n",
      "Epoch 180/250\n",
      " - 0s - loss: 0.3187 - acc: 0.7586 - val_loss: 0.4861 - val_acc: 0.7692\n",
      "Epoch 181/250\n",
      " - 0s - loss: 0.2444 - acc: 0.7586 - val_loss: 0.4870 - val_acc: 0.7692\n",
      "Epoch 182/250\n",
      " - 0s - loss: 0.3257 - acc: 0.7586 - val_loss: 0.4886 - val_acc: 0.7692\n",
      "Epoch 183/250\n",
      " - 0s - loss: 0.2573 - acc: 0.7586 - val_loss: 0.4902 - val_acc: 0.7692\n",
      "Epoch 184/250\n",
      " - 0s - loss: 0.3370 - acc: 0.7586 - val_loss: 0.4915 - val_acc: 0.7692\n",
      "Epoch 185/250\n",
      " - 0s - loss: 0.2841 - acc: 0.7586 - val_loss: 0.4929 - val_acc: 0.7692\n",
      "Epoch 186/250\n",
      " - 0s - loss: 0.2457 - acc: 0.7586 - val_loss: 0.4950 - val_acc: 0.7692\n",
      "Epoch 187/250\n",
      " - 0s - loss: 0.2745 - acc: 0.7586 - val_loss: 0.4973 - val_acc: 0.7692\n",
      "Epoch 188/250\n",
      " - 0s - loss: 0.3190 - acc: 0.7586 - val_loss: 0.4992 - val_acc: 0.7692\n",
      "Epoch 189/250\n",
      " - 0s - loss: 0.2973 - acc: 0.7586 - val_loss: 0.5019 - val_acc: 0.7692\n",
      "Epoch 190/250\n",
      " - 0s - loss: 0.2787 - acc: 0.7586 - val_loss: 0.5039 - val_acc: 0.7692\n",
      "Epoch 191/250\n",
      " - 0s - loss: 0.2821 - acc: 0.7586 - val_loss: 0.5059 - val_acc: 0.7692\n",
      "Epoch 192/250\n",
      " - 0s - loss: 0.2543 - acc: 0.7586 - val_loss: 0.5088 - val_acc: 0.7692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/250\n",
      " - 0s - loss: 0.2766 - acc: 0.7586 - val_loss: 0.5114 - val_acc: 0.7692\n",
      "Epoch 194/250\n",
      " - 0s - loss: 0.2411 - acc: 0.7586 - val_loss: 0.5142 - val_acc: 0.7692\n",
      "Epoch 195/250\n",
      " - 0s - loss: 0.2919 - acc: 0.7586 - val_loss: 0.5166 - val_acc: 0.7692\n",
      "Epoch 196/250\n",
      " - 0s - loss: 0.3250 - acc: 0.7586 - val_loss: 0.5180 - val_acc: 0.7692\n",
      "Epoch 197/250\n",
      " - 0s - loss: 0.3113 - acc: 0.7586 - val_loss: 0.5187 - val_acc: 0.7692\n",
      "Epoch 198/250\n",
      " - 0s - loss: 0.2986 - acc: 0.7586 - val_loss: 0.5186 - val_acc: 0.7692\n",
      "Epoch 199/250\n",
      " - 0s - loss: 0.2704 - acc: 0.7586 - val_loss: 0.5194 - val_acc: 0.7692\n",
      "Epoch 200/250\n",
      " - 0s - loss: 0.2615 - acc: 0.7586 - val_loss: 0.5211 - val_acc: 0.7692\n",
      "Epoch 201/250\n",
      " - 0s - loss: 0.2347 - acc: 0.7586 - val_loss: 0.5236 - val_acc: 0.7692\n",
      "Epoch 202/250\n",
      " - 0s - loss: 0.2730 - acc: 0.7586 - val_loss: 0.5267 - val_acc: 0.7692\n",
      "Epoch 203/250\n",
      " - 0s - loss: 0.2620 - acc: 0.7586 - val_loss: 0.5297 - val_acc: 0.7692\n",
      "Epoch 204/250\n",
      " - 0s - loss: 0.2625 - acc: 0.7586 - val_loss: 0.5326 - val_acc: 0.7692\n",
      "Epoch 205/250\n",
      " - 0s - loss: 0.2701 - acc: 0.7586 - val_loss: 0.5353 - val_acc: 0.7692\n",
      "Epoch 206/250\n",
      " - 0s - loss: 0.2593 - acc: 0.7586 - val_loss: 0.5390 - val_acc: 0.7692\n",
      "Epoch 207/250\n",
      " - 0s - loss: 0.2731 - acc: 0.7586 - val_loss: 0.5423 - val_acc: 0.7692\n",
      "Epoch 208/250\n",
      " - 0s - loss: 0.2544 - acc: 0.7586 - val_loss: 0.5459 - val_acc: 0.7692\n",
      "Epoch 209/250\n",
      " - 0s - loss: 0.2424 - acc: 0.7586 - val_loss: 0.5499 - val_acc: 0.7692\n",
      "Epoch 210/250\n",
      " - 0s - loss: 0.2634 - acc: 0.7586 - val_loss: 0.5535 - val_acc: 0.7692\n",
      "Epoch 211/250\n",
      " - 0s - loss: 0.2588 - acc: 0.7586 - val_loss: 0.5564 - val_acc: 0.7692\n",
      "Epoch 212/250\n",
      " - 0s - loss: 0.2780 - acc: 0.7586 - val_loss: 0.5591 - val_acc: 0.7692\n",
      "Epoch 213/250\n",
      " - 0s - loss: 0.2746 - acc: 0.7586 - val_loss: 0.5624 - val_acc: 0.7692\n",
      "Epoch 214/250\n",
      " - 0s - loss: 0.2617 - acc: 0.7586 - val_loss: 0.5656 - val_acc: 0.7692\n",
      "Epoch 215/250\n",
      " - 0s - loss: 0.2846 - acc: 0.7586 - val_loss: 0.5675 - val_acc: 0.7692\n",
      "Epoch 216/250\n",
      " - 0s - loss: 0.2743 - acc: 0.7586 - val_loss: 0.5681 - val_acc: 0.7692\n",
      "Epoch 217/250\n",
      " - 0s - loss: 0.2578 - acc: 0.7586 - val_loss: 0.5681 - val_acc: 0.7692\n",
      "Epoch 218/250\n",
      " - 0s - loss: 0.2527 - acc: 0.7586 - val_loss: 0.5690 - val_acc: 0.7692\n",
      "Epoch 219/250\n",
      " - 0s - loss: 0.2614 - acc: 0.7586 - val_loss: 0.5698 - val_acc: 0.7692\n",
      "Epoch 220/250\n",
      " - 0s - loss: 0.2747 - acc: 0.7586 - val_loss: 0.5705 - val_acc: 0.7692\n",
      "Epoch 221/250\n",
      " - 0s - loss: 0.2774 - acc: 0.7586 - val_loss: 0.5719 - val_acc: 0.7692\n",
      "Epoch 222/250\n",
      " - 0s - loss: 0.2414 - acc: 0.7586 - val_loss: 0.5745 - val_acc: 0.7692\n",
      "Epoch 223/250\n",
      " - 0s - loss: 0.2975 - acc: 0.7586 - val_loss: 0.5757 - val_acc: 0.7692\n",
      "Epoch 224/250\n",
      " - 0s - loss: 0.2664 - acc: 0.7586 - val_loss: 0.5770 - val_acc: 0.7692\n",
      "Epoch 225/250\n",
      " - 0s - loss: 0.2600 - acc: 0.7586 - val_loss: 0.5791 - val_acc: 0.7692\n",
      "Epoch 226/250\n",
      " - 0s - loss: 0.2635 - acc: 0.7586 - val_loss: 0.5818 - val_acc: 0.7692\n",
      "Epoch 227/250\n",
      " - 0s - loss: 0.2514 - acc: 0.7586 - val_loss: 0.5853 - val_acc: 0.7692\n",
      "Epoch 228/250\n",
      " - 0s - loss: 0.2205 - acc: 0.8966 - val_loss: 0.5892 - val_acc: 0.8846\n",
      "Epoch 229/250\n",
      " - 0s - loss: 0.2493 - acc: 0.9138 - val_loss: 0.5938 - val_acc: 0.8846\n",
      "Epoch 230/250\n",
      " - 0s - loss: 0.2623 - acc: 0.8966 - val_loss: 0.5982 - val_acc: 0.8846\n",
      "Epoch 231/250\n",
      " - 0s - loss: 0.3014 - acc: 0.8103 - val_loss: 0.6002 - val_acc: 0.8846\n",
      "Epoch 232/250\n",
      " - 0s - loss: 0.2615 - acc: 0.8793 - val_loss: 0.6016 - val_acc: 0.8846\n",
      "Epoch 233/250\n",
      " - 0s - loss: 0.2565 - acc: 0.8966 - val_loss: 0.6019 - val_acc: 0.8846\n",
      "Epoch 234/250\n",
      " - 0s - loss: 0.2676 - acc: 0.8793 - val_loss: 0.6027 - val_acc: 0.8846\n",
      "Epoch 235/250\n",
      " - 0s - loss: 0.2477 - acc: 0.8621 - val_loss: 0.6032 - val_acc: 0.8846\n",
      "Epoch 236/250\n",
      " - 0s - loss: 0.2108 - acc: 0.9655 - val_loss: 0.6051 - val_acc: 0.8846\n",
      "Epoch 237/250\n",
      " - 0s - loss: 0.2222 - acc: 0.9310 - val_loss: 0.6067 - val_acc: 0.8846\n",
      "Epoch 238/250\n",
      " - 0s - loss: 0.2323 - acc: 0.9138 - val_loss: 0.6086 - val_acc: 0.8846\n",
      "Epoch 239/250\n",
      " - 0s - loss: 0.2228 - acc: 0.9483 - val_loss: 0.6103 - val_acc: 0.8846\n",
      "Epoch 240/250\n",
      " - 0s - loss: 0.2737 - acc: 0.8448 - val_loss: 0.6113 - val_acc: 0.8846\n",
      "Epoch 241/250\n",
      " - 0s - loss: 0.2524 - acc: 0.9138 - val_loss: 0.6110 - val_acc: 0.8846\n",
      "Epoch 242/250\n",
      " - 0s - loss: 0.2298 - acc: 0.9483 - val_loss: 0.6095 - val_acc: 0.8846\n",
      "Epoch 243/250\n",
      " - 0s - loss: 0.2632 - acc: 0.8448 - val_loss: 0.6076 - val_acc: 0.8846\n",
      "Epoch 244/250\n",
      " - 0s - loss: 0.2273 - acc: 0.8966 - val_loss: 0.6057 - val_acc: 0.8846\n",
      "Epoch 245/250\n",
      " - 0s - loss: 0.2986 - acc: 0.7931 - val_loss: 0.6033 - val_acc: 0.8846\n",
      "Epoch 246/250\n",
      " - 0s - loss: 0.2563 - acc: 0.8966 - val_loss: 0.6022 - val_acc: 0.8846\n",
      "Epoch 247/250\n",
      " - 0s - loss: 0.2257 - acc: 0.9310 - val_loss: 0.6022 - val_acc: 0.8846\n",
      "Epoch 248/250\n",
      " - 0s - loss: 0.2225 - acc: 0.9483 - val_loss: 0.6031 - val_acc: 0.8846\n",
      "Epoch 249/250\n",
      " - 0s - loss: 0.2443 - acc: 0.9138 - val_loss: 0.6048 - val_acc: 0.8846\n",
      "Epoch 250/250\n",
      " - 0s - loss: 0.2206 - acc: 0.9483 - val_loss: 0.6072 - val_acc: 0.8846\n"
     ]
    }
   ],
   "source": [
    "mlp_suggestion = mlp_suggestion(shape)\n",
    "history = mlp_suggestion.fit(X_train, np.asarray(y_train[\"Search\"]).reshape(-1,1),\n",
    "                  validation_data=(X_validation, np.asarray(y_validation[\"Search\"]).reshape(-1,1)),\n",
    "    epochs=250,\n",
    "    workers = 2, use_multiprocessing= True, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_farewell(shape):\n",
    "# define our MLP network\n",
    "    initializer = tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5, seed=42)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=shape, kernel_initializer = initializer, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(2, activation=\"relu\"))\n",
    "# check to see if the regression node should be added\n",
    "    #if regress:\n",
    "    model.add(Dense(1, activation = \"sigmoid\"))\n",
    "    #Compile model\n",
    "    opt = tf.keras.optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss='binary_crossentropy', metrics = [\"accuracy\"], optimizer=opt)\n",
    "# return our model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58 samples, validate on 26 samples\n",
      "Epoch 1/250\n",
      " - 1s - loss: 0.7034 - acc: 0.5862 - val_loss: 0.6971 - val_acc: 0.5769\n",
      "Epoch 2/250\n",
      " - 0s - loss: 0.7015 - acc: 0.6552 - val_loss: 0.6958 - val_acc: 0.6154\n",
      "Epoch 3/250\n",
      " - 0s - loss: 0.6984 - acc: 0.6724 - val_loss: 0.6946 - val_acc: 0.6538\n",
      "Epoch 4/250\n",
      " - 0s - loss: 0.6963 - acc: 0.6897 - val_loss: 0.6936 - val_acc: 0.6538\n",
      "Epoch 5/250\n",
      " - 0s - loss: 0.6951 - acc: 0.6552 - val_loss: 0.6927 - val_acc: 0.6538\n",
      "Epoch 6/250\n",
      " - 0s - loss: 0.6860 - acc: 0.8103 - val_loss: 0.6918 - val_acc: 0.6538\n",
      "Epoch 7/250\n",
      " - 0s - loss: 0.6951 - acc: 0.6897 - val_loss: 0.6908 - val_acc: 0.6538\n",
      "Epoch 8/250\n",
      " - 0s - loss: 0.6961 - acc: 0.6552 - val_loss: 0.6898 - val_acc: 0.7692\n",
      "Epoch 9/250\n",
      " - 0s - loss: 0.6909 - acc: 0.6897 - val_loss: 0.6888 - val_acc: 0.7692\n",
      "Epoch 10/250\n",
      " - 0s - loss: 0.6890 - acc: 0.7586 - val_loss: 0.6882 - val_acc: 0.7692\n",
      "Epoch 11/250\n",
      " - 0s - loss: 0.6888 - acc: 0.7241 - val_loss: 0.6877 - val_acc: 0.7692\n",
      "Epoch 12/250\n",
      " - 0s - loss: 0.6891 - acc: 0.7414 - val_loss: 0.6871 - val_acc: 0.7692\n",
      "Epoch 13/250\n",
      " - 0s - loss: 0.6887 - acc: 0.7069 - val_loss: 0.6866 - val_acc: 0.7692\n",
      "Epoch 14/250\n",
      " - 0s - loss: 0.6913 - acc: 0.6724 - val_loss: 0.6861 - val_acc: 0.7692\n",
      "Epoch 15/250\n",
      " - 0s - loss: 0.6880 - acc: 0.6897 - val_loss: 0.6856 - val_acc: 0.7692\n",
      "Epoch 16/250\n",
      " - 0s - loss: 0.6854 - acc: 0.7586 - val_loss: 0.6851 - val_acc: 0.7692\n",
      "Epoch 17/250\n",
      " - 0s - loss: 0.6871 - acc: 0.7241 - val_loss: 0.6846 - val_acc: 0.7692\n",
      "Epoch 18/250\n",
      " - 0s - loss: 0.6848 - acc: 0.7586 - val_loss: 0.6840 - val_acc: 0.7692\n",
      "Epoch 19/250\n",
      " - 0s - loss: 0.6882 - acc: 0.7241 - val_loss: 0.6835 - val_acc: 0.7692\n",
      "Epoch 20/250\n",
      " - 0s - loss: 0.6837 - acc: 0.7586 - val_loss: 0.6830 - val_acc: 0.7692\n",
      "Epoch 21/250\n",
      " - 0s - loss: 0.6832 - acc: 0.7414 - val_loss: 0.6825 - val_acc: 0.7692\n",
      "Epoch 22/250\n",
      " - 0s - loss: 0.6828 - acc: 0.7586 - val_loss: 0.6820 - val_acc: 0.7692\n",
      "Epoch 23/250\n",
      " - 0s - loss: 0.6811 - acc: 0.7586 - val_loss: 0.6815 - val_acc: 0.7692\n",
      "Epoch 24/250\n",
      " - 0s - loss: 0.6820 - acc: 0.7586 - val_loss: 0.6810 - val_acc: 0.7692\n",
      "Epoch 25/250\n",
      " - 0s - loss: 0.6818 - acc: 0.7414 - val_loss: 0.6805 - val_acc: 0.7692\n",
      "Epoch 26/250\n",
      " - 0s - loss: 0.6809 - acc: 0.7586 - val_loss: 0.6800 - val_acc: 0.7692\n",
      "Epoch 27/250\n",
      " - 0s - loss: 0.6782 - acc: 0.8103 - val_loss: 0.6795 - val_acc: 0.7692\n",
      "Epoch 28/250\n",
      " - 0s - loss: 0.6789 - acc: 0.7759 - val_loss: 0.6790 - val_acc: 0.7692\n",
      "Epoch 29/250\n",
      " - 0s - loss: 0.6797 - acc: 0.7414 - val_loss: 0.6785 - val_acc: 0.7692\n",
      "Epoch 30/250\n",
      " - 0s - loss: 0.6799 - acc: 0.7586 - val_loss: 0.6781 - val_acc: 0.7692\n",
      "Epoch 31/250\n",
      " - 0s - loss: 0.6783 - acc: 0.7586 - val_loss: 0.6776 - val_acc: 0.7692\n",
      "Epoch 32/250\n",
      " - 0s - loss: 0.6792 - acc: 0.7414 - val_loss: 0.6771 - val_acc: 0.7692\n",
      "Epoch 33/250\n",
      " - 0s - loss: 0.6774 - acc: 0.7586 - val_loss: 0.6766 - val_acc: 0.7692\n",
      "Epoch 34/250\n",
      " - 0s - loss: 0.6766 - acc: 0.7759 - val_loss: 0.6761 - val_acc: 0.7692\n",
      "Epoch 35/250\n",
      " - 0s - loss: 0.6768 - acc: 0.7586 - val_loss: 0.6756 - val_acc: 0.7692\n",
      "Epoch 36/250\n",
      " - 0s - loss: 0.6767 - acc: 0.7586 - val_loss: 0.6752 - val_acc: 0.7692\n",
      "Epoch 37/250\n",
      " - 0s - loss: 0.6752 - acc: 0.7759 - val_loss: 0.6747 - val_acc: 0.7692\n",
      "Epoch 38/250\n",
      " - 0s - loss: 0.6780 - acc: 0.7414 - val_loss: 0.6742 - val_acc: 0.7692\n",
      "Epoch 39/250\n",
      " - 0s - loss: 0.6747 - acc: 0.7586 - val_loss: 0.6737 - val_acc: 0.7692\n",
      "Epoch 40/250\n",
      " - 0s - loss: 0.6736 - acc: 0.7759 - val_loss: 0.6733 - val_acc: 0.7692\n",
      "Epoch 41/250\n",
      " - 0s - loss: 0.6740 - acc: 0.7586 - val_loss: 0.6728 - val_acc: 0.7692\n",
      "Epoch 42/250\n",
      " - 0s - loss: 0.6743 - acc: 0.7414 - val_loss: 0.6723 - val_acc: 0.7692\n",
      "Epoch 43/250\n",
      " - 0s - loss: 0.6722 - acc: 0.7759 - val_loss: 0.6718 - val_acc: 0.7692\n",
      "Epoch 44/250\n",
      " - 0s - loss: 0.6726 - acc: 0.7586 - val_loss: 0.6714 - val_acc: 0.7692\n",
      "Epoch 45/250\n",
      " - 0s - loss: 0.6715 - acc: 0.7759 - val_loss: 0.6709 - val_acc: 0.7692\n",
      "Epoch 46/250\n",
      " - 0s - loss: 0.6710 - acc: 0.7586 - val_loss: 0.6705 - val_acc: 0.7692\n",
      "Epoch 47/250\n",
      " - 0s - loss: 0.6711 - acc: 0.7586 - val_loss: 0.6700 - val_acc: 0.7692\n",
      "Epoch 48/250\n",
      " - 0s - loss: 0.6717 - acc: 0.7586 - val_loss: 0.6695 - val_acc: 0.7692\n",
      "Epoch 49/250\n",
      " - 0s - loss: 0.6716 - acc: 0.7414 - val_loss: 0.6691 - val_acc: 0.7692\n",
      "Epoch 50/250\n",
      " - 0s - loss: 0.6698 - acc: 0.7586 - val_loss: 0.6686 - val_acc: 0.7692\n",
      "Epoch 51/250\n",
      " - 0s - loss: 0.6664 - acc: 0.7931 - val_loss: 0.6682 - val_acc: 0.7692\n",
      "Epoch 52/250\n",
      " - 0s - loss: 0.6695 - acc: 0.7586 - val_loss: 0.6677 - val_acc: 0.7692\n",
      "Epoch 53/250\n",
      " - 0s - loss: 0.6686 - acc: 0.7586 - val_loss: 0.6673 - val_acc: 0.7692\n",
      "Epoch 54/250\n",
      " - 0s - loss: 0.6684 - acc: 0.7414 - val_loss: 0.6668 - val_acc: 0.7692\n",
      "Epoch 55/250\n",
      " - 0s - loss: 0.6664 - acc: 0.7586 - val_loss: 0.6664 - val_acc: 0.7692\n",
      "Epoch 56/250\n",
      " - 0s - loss: 0.6683 - acc: 0.7414 - val_loss: 0.6659 - val_acc: 0.7692\n",
      "Epoch 57/250\n",
      " - 0s - loss: 0.6658 - acc: 0.7759 - val_loss: 0.6655 - val_acc: 0.7692\n",
      "Epoch 58/250\n",
      " - 0s - loss: 0.6663 - acc: 0.7586 - val_loss: 0.6650 - val_acc: 0.7692\n",
      "Epoch 59/250\n",
      " - 0s - loss: 0.6652 - acc: 0.7586 - val_loss: 0.6645 - val_acc: 0.7692\n",
      "Epoch 60/250\n",
      " - 0s - loss: 0.6653 - acc: 0.7586 - val_loss: 0.6641 - val_acc: 0.7692\n",
      "Epoch 61/250\n",
      " - 0s - loss: 0.6654 - acc: 0.7586 - val_loss: 0.6637 - val_acc: 0.7692\n",
      "Epoch 62/250\n",
      " - 0s - loss: 0.6662 - acc: 0.7414 - val_loss: 0.6632 - val_acc: 0.7692\n",
      "Epoch 63/250\n",
      " - 0s - loss: 0.6635 - acc: 0.7759 - val_loss: 0.6628 - val_acc: 0.7692\n"
     ]
    }
   ],
   "source": [
    "mlp_farewell = mlp_farewell(shape)\n",
    "history = mlp_farewell.fit(X_train, np.asarray(y_train[\"Search\"]).reshape(-1,1),\n",
    "                  validation_data=(X_validation, np.asarray(y_validation[\"Search\"]).reshape(-1,1)),\n",
    "    epochs=250,\n",
    "    workers = 2, use_multiprocessing= True, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_greeting.save('model_greetings.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHATBOT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot():\n",
    "    \n",
    "    input_text = input()\n",
    "    \n",
    "    test = pd.DataFrame(data = {'Sentence': [input_text]})\n",
    "    df_test_proc, test_proc = processing(test, cv = cv)\n",
    "\n",
    "    gret_prob = mlp_greeting.predict_proba(test_proc)\n",
    "    search_prob = mlp_search.predict_proba(test_proc)\n",
    "    sugg_prob = mlp_suggestion.predict_proba(test_proc)\n",
    "    \n",
    "    probs = [gret_prob, search_prob, sugg_prob]\n",
    "    idx = np.argmax(probs)\n",
    "    \n",
    "    if idx == 0:\n",
    "        print(\"Esto es un saludo\")\n",
    "    elif idx == 1:\n",
    "        print(\"Esto es una búsqueda\")\n",
    "    else:\n",
    "        print(\"Esto es una sugerencia\")\n",
    "        \n",
    "#     print('¿Hemos acertado?')\n",
    "    \n",
    "#     respuesta = input()\n",
    "#     if (respuesta == 'No' or respuesta == 'no'):\n",
    "#         probs = np.delete(probs, idx)\n",
    "#         idx_2 = np.argmax(probs)\n",
    "        \n",
    "#         if idx == 0:\n",
    "#             if idx_2 == 0:\n",
    "#                 print(\"Esto es una búsqueda\")\n",
    "#             else:\n",
    "#                 print(\"Esto es una sugerencia\")\n",
    "#         elif idx == 1:\n",
    "#             if idx_2 == 0:\n",
    "#                 print(\"Esto es un saludo\")\n",
    "#             else:\n",
    "#                 print(\"Esto es una sugerencia\")\n",
    "#         else:\n",
    "#             if idx_2 == 0:\n",
    "#                 print(\"Esto es un saludo\")\n",
    "#             else:\n",
    "#                 print(\"Esto es una búsqueda\")\n",
    "#     else:\n",
    "#         print('¡Genial! ¡Hemos acertado!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me things related to Napoleon\n",
      "Esto es un saludo\n"
     ]
    }
   ],
   "source": [
    "chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Correctamente\n",
    " * La mayoría de saludos\n",
    " * I want to know about Data Science (búsqueda)\n",
    " * I want to know things connected to la Sagrada Familia (sugerencia)\n",
    " * Watcha doing'? (saludo)\n",
    " * Tell me things related to Data Science (sugerencia)\n",
    "\n",
    "* Incorrectamente\n",
    " * What is Data Science (sugerencia)\n",
    " * Tell me things related to Napoleon (saludo)\n",
    " * Who is Cristobal Colon (saludo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Opciones\n",
    " * Mostrar un comando *!options* que te de las opciones para hacer.\n",
    " * Detectar un intent\n",
    " * Mostrarlo al inicio\n",
    "\n",
    "\n",
    "* Headers\n",
    " * Dar las opciones una vez se detecta intent de búsqueda búsqueda\n",
    "   * *What is data science?* Y mostrarle los heades para qué elija qué queire saber \n",
    " * Intent nuevo de header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
